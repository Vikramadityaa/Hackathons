{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.emsemble'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-390-cfe7a985a9c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memsemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.emsemble'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import naive_bayes,model_selection,metrics\n",
    "import xgboost as xgb\n",
    "import string\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.emsemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>On maximizing the fundamental frequency of the...</td>\n",
       "      <td>Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>On the rotation period and shape of the hyperb...</td>\n",
       "      <td>We observed the newly discovered hyperbolic ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Adverse effects of polymer coating on heat tra...</td>\n",
       "      <td>The ability of metallic nanoparticles to sup...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SPH calculations of Mars-scale collisions: the...</td>\n",
       "      <td>We model large-scale ($\\approx$2000km) impac...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>$\\mathcal{R}_{0}$ fails to predict the outbrea...</td>\n",
       "      <td>Time varying susceptibility of host at indiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>A global sensitivity analysis and reduced orde...</td>\n",
       "      <td>We present a systematic global sensitivity a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Role-separating ordering in social dilemmas co...</td>\n",
       "      <td>\"Three is a crowd\" is an old proverb that ap...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Dynamics of exciton magnetic polarons in CdMnS...</td>\n",
       "      <td>We study the exciton magnetic polaron (EMP) ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>On Varieties of Ordered Automata</td>\n",
       "      <td>The classical Eilenberg correspondence, base...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Direct Evidence of Spontaneous Abrikosov Vorte...</td>\n",
       "      <td>Using low-temperature Magnetic Force Microsc...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>A rank 18 Waring decomposition of $sM_{\\langle...</td>\n",
       "      <td>The recent discovery that the exponent of ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>The PdBI Arcsecond Whirlpool Survey (PAWS). Th...</td>\n",
       "      <td>The process that leads to the formation of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Higher structure in the unstable Adams spectra...</td>\n",
       "      <td>We describe a variant construction of the un...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Comparing Covariate Prioritization via Matchin...</td>\n",
       "      <td>When investigators seek to estimate causal e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Acoustic Impedance Calculation via Numerical S...</td>\n",
       "      <td>Assigning homogeneous boundary conditions, s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Deciphering noise amplification and reduction ...</td>\n",
       "      <td>The impact of random fluctuations on the dyn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Many-Body Localization: Stability and Instability</td>\n",
       "      <td>Rare regions with weak disorder (Griffiths r...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Fault Detection and Isolation Tools (FDITOOLS)...</td>\n",
       "      <td>The Fault Detection and Isolation Tools (FDI...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Complexity of Deciding Detectability in Discre...</td>\n",
       "      <td>Detectability of discrete event systems (DES...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>The Knaster-Tarski theorem versus monotone non...</td>\n",
       "      <td>Let $X$ be a partially ordered set with the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Efficient methods for computing integrals in e...</td>\n",
       "      <td>Efficient methods are proposed, for computin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Diffraction-Aware Sound Localization for a Non...</td>\n",
       "      <td>We present a novel sound localization algori...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Jacob's ladders, crossbreeding in the set of $...</td>\n",
       "      <td>In this paper we introduce the notion of $\\z...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Minimax Estimation of the $L_1$ Distance</td>\n",
       "      <td>We consider the problem of estimating the $L...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Density large deviations for multidimensional ...</td>\n",
       "      <td>We investigate the density large deviation f...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>mixup: Beyond Empirical Risk Minimization</td>\n",
       "      <td>Large deep neural networks are powerful, but...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Equality of the usual definitions of Brakke flow</td>\n",
       "      <td>In 1978 Brakke introduced the mean curvature...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Dynamic Base Station Repositioning to Improve ...</td>\n",
       "      <td>With recent advancements in drone technology...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>An Unsupervised Homogenization Pipeline for Cl...</td>\n",
       "      <td>Electronic health records (EHR) contain a la...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>Deep Neural Network Optimized to Resistive Mem...</td>\n",
       "      <td>Artificial Neural Network computation relies...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>Rate-Distortion Region of a Gray-Wyner Model w...</td>\n",
       "      <td>In this work, we establish a full single-let...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>Fourier-based numerical approximation of the W...</td>\n",
       "      <td>This work discusses the numerical approximat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>Design Decisions for Weave: A Real-Time Web-ba...</td>\n",
       "      <td>There are many web-based visualization syste...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>Suzaku Analysis of the Supernova Remnant G306....</td>\n",
       "      <td>We present an investigation of the supernova...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>Japanese Sentiment Classification using a Tree...</td>\n",
       "      <td>Previous approaches to training syntax-based...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>Covariances, Robustness, and Variational Bayes</td>\n",
       "      <td>Mean-field Variational Bayes (MFVB) is an ap...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>Are multi-factor Gaussian term structure model...</td>\n",
       "      <td>In this paper, we empirically study models f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>Probing valley filtering effect by Andreev ref...</td>\n",
       "      <td>Ballistic point contact (BPC) with zigzag ed...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>Generalized Approximate Message-Passing Decode...</td>\n",
       "      <td>Sparse superposition (SS) codes were origina...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>LAAIR: A Layered Architecture for Autonomous I...</td>\n",
       "      <td>When developing general purpose robots, the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>3D Human Pose Estimation in RGBD Images for Ro...</td>\n",
       "      <td>We propose an approach to estimate 3D human ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>Simultaneous non-vanishing for Dirichlet L-fun...</td>\n",
       "      <td>We extend the work of Fouvry, Kowalski and M...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>Wehrl Entropy Based Quantification of Nonclass...</td>\n",
       "      <td>Nonclassical states of a quantized light are...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>Attention-based Natural Language Person Retrieval</td>\n",
       "      <td>Following the recent progress in image class...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>Large Scale Automated Forecasting for Monitori...</td>\n",
       "      <td>Real time large scale streaming data pose ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                              TITLE  \\\n",
       "0    1        Reconstructing Subject-Specific Effect Maps   \n",
       "1    2                 Rotation Invariance Neural Network   \n",
       "2    3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3    4  A finite element approximation for the stochas...   \n",
       "4    5  Comparative study of Discrete Wavelet Transfor...   \n",
       "5    6  On maximizing the fundamental frequency of the...   \n",
       "6    7  On the rotation period and shape of the hyperb...   \n",
       "7    8  Adverse effects of polymer coating on heat tra...   \n",
       "8    9  SPH calculations of Mars-scale collisions: the...   \n",
       "9   10  $\\mathcal{R}_{0}$ fails to predict the outbrea...   \n",
       "10  11  A global sensitivity analysis and reduced orde...   \n",
       "11  12  Role-separating ordering in social dilemmas co...   \n",
       "12  13  Dynamics of exciton magnetic polarons in CdMnS...   \n",
       "13  14                   On Varieties of Ordered Automata   \n",
       "14  15  Direct Evidence of Spontaneous Abrikosov Vorte...   \n",
       "15  16  A rank 18 Waring decomposition of $sM_{\\langle...   \n",
       "16  17  The PdBI Arcsecond Whirlpool Survey (PAWS). Th...   \n",
       "17  18  Higher structure in the unstable Adams spectra...   \n",
       "18  19  Comparing Covariate Prioritization via Matchin...   \n",
       "19  20  Acoustic Impedance Calculation via Numerical S...   \n",
       "20  21  Deciphering noise amplification and reduction ...   \n",
       "21  22  Many-Body Localization: Stability and Instability   \n",
       "22  23  Fault Detection and Isolation Tools (FDITOOLS)...   \n",
       "23  24  Complexity of Deciding Detectability in Discre...   \n",
       "24  25  The Knaster-Tarski theorem versus monotone non...   \n",
       "25  26  Efficient methods for computing integrals in e...   \n",
       "26  27  Diffraction-Aware Sound Localization for a Non...   \n",
       "27  28  Jacob's ladders, crossbreeding in the set of $...   \n",
       "28  29           Minimax Estimation of the $L_1$ Distance   \n",
       "29  30  Density large deviations for multidimensional ...   \n",
       "30  31          mixup: Beyond Empirical Risk Minimization   \n",
       "31  32   Equality of the usual definitions of Brakke flow   \n",
       "32  33  Dynamic Base Station Repositioning to Improve ...   \n",
       "33  34  An Unsupervised Homogenization Pipeline for Cl...   \n",
       "34  35  Deep Neural Network Optimized to Resistive Mem...   \n",
       "35  36  Rate-Distortion Region of a Gray-Wyner Model w...   \n",
       "36  37  Fourier-based numerical approximation of the W...   \n",
       "37  38  Design Decisions for Weave: A Real-Time Web-ba...   \n",
       "38  39  Suzaku Analysis of the Supernova Remnant G306....   \n",
       "39  40  Japanese Sentiment Classification using a Tree...   \n",
       "40  41     Covariances, Robustness, and Variational Bayes   \n",
       "41  42  Are multi-factor Gaussian term structure model...   \n",
       "42  43  Probing valley filtering effect by Andreev ref...   \n",
       "43  44  Generalized Approximate Message-Passing Decode...   \n",
       "44  45  LAAIR: A Layered Architecture for Autonomous I...   \n",
       "45  46  3D Human Pose Estimation in RGBD Images for Ro...   \n",
       "46  47  Simultaneous non-vanishing for Dirichlet L-fun...   \n",
       "47  48  Wehrl Entropy Based Quantification of Nonclass...   \n",
       "48  49  Attention-based Natural Language Person Retrieval   \n",
       "49  50  Large Scale Automated Forecasting for Monitori...   \n",
       "\n",
       "                                             ABSTRACT  Computer Science  \\\n",
       "0     Predictive models allow subject-specific inf...                 1   \n",
       "1     Rotation invariance and translation invarian...                 1   \n",
       "2     We introduce and develop the notion of spher...                 0   \n",
       "3     The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4     Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "5     Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...                 0   \n",
       "6     We observed the newly discovered hyperbolic ...                 0   \n",
       "7     The ability of metallic nanoparticles to sup...                 0   \n",
       "8     We model large-scale ($\\approx$2000km) impac...                 0   \n",
       "9     Time varying susceptibility of host at indiv...                 0   \n",
       "10    We present a systematic global sensitivity a...                 1   \n",
       "11    \"Three is a crowd\" is an old proverb that ap...                 0   \n",
       "12    We study the exciton magnetic polaron (EMP) ...                 0   \n",
       "13    The classical Eilenberg correspondence, base...                 1   \n",
       "14    Using low-temperature Magnetic Force Microsc...                 0   \n",
       "15    The recent discovery that the exponent of ma...                 0   \n",
       "16    The process that leads to the formation of t...                 0   \n",
       "17    We describe a variant construction of the un...                 0   \n",
       "18    When investigators seek to estimate causal e...                 0   \n",
       "19    Assigning homogeneous boundary conditions, s...                 0   \n",
       "20    The impact of random fluctuations on the dyn...                 0   \n",
       "21    Rare regions with weak disorder (Griffiths r...                 0   \n",
       "22    The Fault Detection and Isolation Tools (FDI...                 1   \n",
       "23    Detectability of discrete event systems (DES...                 1   \n",
       "24    Let $X$ be a partially ordered set with the ...                 0   \n",
       "25    Efficient methods are proposed, for computin...                 0   \n",
       "26    We present a novel sound localization algori...                 1   \n",
       "27    In this paper we introduce the notion of $\\z...                 0   \n",
       "28    We consider the problem of estimating the $L...                 0   \n",
       "29    We investigate the density large deviation f...                 0   \n",
       "30    Large deep neural networks are powerful, but...                 1   \n",
       "31    In 1978 Brakke introduced the mean curvature...                 0   \n",
       "32    With recent advancements in drone technology...                 1   \n",
       "33    Electronic health records (EHR) contain a la...                 0   \n",
       "34    Artificial Neural Network computation relies...                 1   \n",
       "35    In this work, we establish a full single-let...                 1   \n",
       "36    This work discusses the numerical approximat...                 0   \n",
       "37    There are many web-based visualization syste...                 1   \n",
       "38    We present an investigation of the supernova...                 0   \n",
       "39    Previous approaches to training syntax-based...                 1   \n",
       "40    Mean-field Variational Bayes (MFVB) is an ap...                 0   \n",
       "41    In this paper, we empirically study models f...                 0   \n",
       "42    Ballistic point contact (BPC) with zigzag ed...                 0   \n",
       "43    Sparse superposition (SS) codes were origina...                 1   \n",
       "44    When developing general purpose robots, the ...                 1   \n",
       "45    We propose an approach to estimate 3D human ...                 1   \n",
       "46    We extend the work of Fouvry, Kowalski and M...                 0   \n",
       "47    Nonclassical states of a quantized light are...                 1   \n",
       "48    Following the recent progress in image class...                 1   \n",
       "49    Real time large scale streaming data pose ma...                 0   \n",
       "\n",
       "    Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0         0            0           0                     0   \n",
       "1         0            0           0                     0   \n",
       "2         0            1           0                     0   \n",
       "3         0            1           0                     0   \n",
       "4         0            0           1                     0   \n",
       "5         0            1           0                     0   \n",
       "6         1            0           0                     0   \n",
       "7         1            0           0                     0   \n",
       "8         1            0           0                     0   \n",
       "9         0            0           0                     1   \n",
       "10        0            0           0                     0   \n",
       "11        1            0           0                     0   \n",
       "12        1            0           0                     0   \n",
       "13        0            0           0                     0   \n",
       "14        1            0           0                     0   \n",
       "15        0            1           0                     0   \n",
       "16        1            0           0                     0   \n",
       "17        0            1           0                     0   \n",
       "18        0            0           1                     0   \n",
       "19        1            0           0                     0   \n",
       "20        0            0           0                     1   \n",
       "21        1            1           0                     0   \n",
       "22        0            0           0                     0   \n",
       "23        0            0           0                     0   \n",
       "24        0            1           0                     0   \n",
       "25        1            0           0                     0   \n",
       "26        0            0           0                     0   \n",
       "27        0            1           0                     0   \n",
       "28        0            1           1                     0   \n",
       "29        1            1           0                     0   \n",
       "30        0            0           1                     0   \n",
       "31        0            1           0                     0   \n",
       "32        0            0           0                     0   \n",
       "33        0            0           0                     1   \n",
       "34        0            0           0                     0   \n",
       "35        0            1           0                     0   \n",
       "36        1            0           0                     0   \n",
       "37        0            0           0                     0   \n",
       "38        1            0           0                     0   \n",
       "39        0            0           0                     0   \n",
       "40        0            0           1                     0   \n",
       "41        0            0           0                     0   \n",
       "42        1            0           0                     0   \n",
       "43        0            1           0                     0   \n",
       "44        0            0           0                     0   \n",
       "45        0            0           0                     0   \n",
       "46        0            1           0                     0   \n",
       "47        1            0           0                     0   \n",
       "48        0            0           0                     0   \n",
       "49        0            0           1                     0   \n",
       "\n",
       "    Quantitative Finance  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "5                      0  \n",
       "6                      0  \n",
       "7                      0  \n",
       "8                      0  \n",
       "9                      0  \n",
       "10                     0  \n",
       "11                     0  \n",
       "12                     0  \n",
       "13                     0  \n",
       "14                     0  \n",
       "15                     0  \n",
       "16                     0  \n",
       "17                     0  \n",
       "18                     0  \n",
       "19                     0  \n",
       "20                     0  \n",
       "21                     0  \n",
       "22                     0  \n",
       "23                     0  \n",
       "24                     0  \n",
       "25                     0  \n",
       "26                     0  \n",
       "27                     0  \n",
       "28                     0  \n",
       "29                     0  \n",
       "30                     0  \n",
       "31                     0  \n",
       "32                     0  \n",
       "33                     0  \n",
       "34                     0  \n",
       "35                     0  \n",
       "36                     0  \n",
       "37                     0  \n",
       "38                     0  \n",
       "39                     0  \n",
       "40                     0  \n",
       "41                     1  \n",
       "42                     0  \n",
       "43                     0  \n",
       "44                     0  \n",
       "45                     0  \n",
       "46                     0  \n",
       "47                     0  \n",
       "48                     0  \n",
       "49                     0  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20972, 9)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# train['target'] = 0\n",
    "# for col in ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']:\n",
    "#      train[col] = train[col]*i\n",
    "#      i+=1\n",
    "# for j in range(0,train.shape[0]):\n",
    "#     for col in ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']:\n",
    "#         if train.loc[j,[col]].values!=0:\n",
    "#             train.loc[j,['target']] = train.loc[j,[col]].values\n",
    "# # f = lambda x: \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>On maximizing the fundamental frequency of the...</td>\n",
       "      <td>Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>On the rotation period and shape of the hyperb...</td>\n",
       "      <td>We observed the newly discovered hyperbolic ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Adverse effects of polymer coating on heat tra...</td>\n",
       "      <td>The ability of metallic nanoparticles to sup...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SPH calculations of Mars-scale collisions: the...</td>\n",
       "      <td>We model large-scale ($\\approx$2000km) impac...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>$\\mathcal{R}_{0}$ fails to predict the outbrea...</td>\n",
       "      <td>Time varying susceptibility of host at indiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              TITLE  \\\n",
       "0   1        Reconstructing Subject-Specific Effect Maps   \n",
       "1   2                 Rotation Invariance Neural Network   \n",
       "2   3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3   4  A finite element approximation for the stochas...   \n",
       "4   5  Comparative study of Discrete Wavelet Transfor...   \n",
       "5   6  On maximizing the fundamental frequency of the...   \n",
       "6   7  On the rotation period and shape of the hyperb...   \n",
       "7   8  Adverse effects of polymer coating on heat tra...   \n",
       "8   9  SPH calculations of Mars-scale collisions: the...   \n",
       "9  10  $\\mathcal{R}_{0}$ fails to predict the outbrea...   \n",
       "\n",
       "                                            ABSTRACT  Computer Science  \\\n",
       "0    Predictive models allow subject-specific inf...                 1   \n",
       "1    Rotation invariance and translation invarian...                 1   \n",
       "2    We introduce and develop the notion of spher...                 0   \n",
       "3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4    Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "5    Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...                 0   \n",
       "6    We observed the newly discovered hyperbolic ...                 0   \n",
       "7    The ability of metallic nanoparticles to sup...                 0   \n",
       "8    We model large-scale ($\\approx$2000km) impac...                 0   \n",
       "9    Time varying susceptibility of host at indiv...                 0   \n",
       "\n",
       "   Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0        0            0           0                     0   \n",
       "1        0            0           0                     0   \n",
       "2        0            1           0                     0   \n",
       "3        0            1           0                     0   \n",
       "4        0            0           1                     0   \n",
       "5        0            1           0                     0   \n",
       "6        1            0           0                     0   \n",
       "7        1            0           0                     0   \n",
       "8        1            0           0                     0   \n",
       "9        0            0           0                     1   \n",
       "\n",
       "   Quantitative Finance  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "5                     0  \n",
       "6                     0  \n",
       "7                     0  \n",
       "8                     0  \n",
       "9                     0  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['num_words_title'] = train['TITLE'].apply(lambda x: len(str(x).split()))\n",
    "test['num_words_title'] = test['TITLE'].apply(lambda x: len(str(x).split()))\n",
    "train['num_words_abs'] = train['ABSTRACT'].apply(lambda x: len(str(x).split()))\n",
    "test['num_words_abs'] = test['ABSTRACT'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['num_unique_title'] = train['TITLE'].apply(lambda x: len(set(str(x).split())))\n",
    "test['num_unique_title'] = test['TITLE'].apply(lambda x: len(set(str(x).split())))\n",
    "train['num_unique_abs'] = train['ABSTRACT'].apply(lambda x: len(set(str(x).split())))\n",
    "test['num_unique_abs'] = test['ABSTRACT'].apply(lambda x: len(set(str(x).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['word_upper_title'] = train['TITLE'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test['word_upper_title'] = test['TITLE'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "train['word_upper_abs'] = train['ABSTRACT'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test['word_upper_abs'] = test['ABSTRACT'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['word_title_title'] = train['TITLE'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test['word_title_title'] = test['TITLE'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "train['word_title_abs'] = train['ABSTRACT'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test['word_title_abs'] = test['ABSTRACT'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"num_punctuations\"] =train['ABSTRACT'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_punctuations\"] =test['ABSTRACT'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"mean_word_len_tit\"] = train[\"TITLE\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test[\"mean_word_len_tit\"] = test['TITLE'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "train[\"mean_word_len_abs\"] = train[\"ABSTRACT\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test[\"mean_word_len_abs\"] = test['ABSTRACT'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"num_chars\"] = train[\"ABSTRACT\"].apply(lambda x: len(str(x)))\n",
    "test[\"num_chars\"] = test[\"ABSTRACT\"].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec_t = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf_t = tfidf_vec_t.fit_transform(train['TITLE'].values.tolist() + test['TITLE'].values.tolist())\n",
    "train_tfidf_t = tfidf_vec_t.transform(train['TITLE'].values.tolist())\n",
    "test_tfidf_t = tfidf_vec_t.transform(test['TITLE'].values.tolist())\n",
    "tfidf_vec_a = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf_a = tfidf_vec_a.fit_transform(train['ABSTRACT'].values.tolist() + test['ABSTRACT'].values.tolist())\n",
    "train_tfidf_a = tfidf_vec_a.transform(train['ABSTRACT'].values.tolist())\n",
    "test_tfidf_a = tfidf_vec_a.transform(test['ABSTRACT'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t = 10\n",
    "n_a =25\n",
    "svd_t = TruncatedSVD(n_components=n_t,algorithm='arpack')\n",
    "svd_a = TruncatedSVD(n_components=n_a,algorithm='arpack')\n",
    "svd_t.fit(full_tfidf_t)\n",
    "svd_a.fit(full_tfidf_a)\n",
    "train_svd_t = pd.DataFrame(svd_t.transform(train_tfidf_t))\n",
    "test_svd_t = pd.DataFrame(svd_t.transform(test_tfidf_t))\n",
    "train_svd_a = pd.DataFrame(svd_a.transform(train_tfidf_a))\n",
    "test_svd_a = pd.DataFrame(svd_a.transform(test_tfidf_a))\n",
    "train_svd_t.columns = ['svd_word_t'+str(i) for i in range(n_t)]\n",
    "test_svd_t.columns = ['svd_word_t'+str(i) for i in range(n_t)]\n",
    "train = pd.concat([train, train_svd_t], axis=1)\n",
    "test = pd.concat([test, test_svd_t], axis=1)\n",
    "train_svd_a.columns = ['svd_word_a'+str(i) for i in range(n_a)]\n",
    "test_svd_a.columns = ['svd_word_a'+str(i) for i in range(n_a)]\n",
    "train = pd.concat([train, train_svd_a], axis=1)\n",
    "test = pd.concat([test, test_svd_a], axis=1)\n",
    "# del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnb(train_x,train_y,test_x,test_y,test_x2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_x,train_y)\n",
    "    pred_test_y = model.predict_proba(test_x)\n",
    "    pred_test_y2 = model.predict_proba(test_x2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec_t = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec_t.fit(train['TITLE'].values.tolist() + test['TITLE'].values.tolist())\n",
    "train_tfidf_t = tfidf_vec_t.transform(train['TITLE'].values.tolist())\n",
    "test_tfidf_t = tfidf_vec_t.transform(test['TITLE'].values.tolist())\n",
    "tfidf_vec_a = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec_a.fit(train['ABSTRACT'].values.tolist() + test['ABSTRACT'].values.tolist())\n",
    "train_tfidf_a = tfidf_vec_a.transform(train['ABSTRACT'].values.tolist())\n",
    "test_tfidf_a = tfidf_vec_a.transform(test['ABSTRACT'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']:\n",
    "    pred_full_test_t = 0\n",
    "    pred_train_t = np.zeros([train.shape[0], 2])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(train):\n",
    "        dev_X, val_X = train_tfidf_t[dev_index], train_tfidf_t[val_index]\n",
    "        dev_y, val_y = train.loc[dev_index,col], train.loc[val_index,col]\n",
    "        pred_val_y, pred_test_y, model = mnb(dev_X, dev_y, val_X, val_y, test_tfidf_t)\n",
    "        pred_full_test_t = pred_full_test_t + pred_test_y\n",
    "        pred_train_t[val_index,:] = pred_val_y\n",
    "#         cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    \n",
    "    pred_full_test_t = pred_full_test_t / 5\n",
    "    \n",
    "    pred_full_test_a = 0\n",
    "    pred_train_a = np.zeros([train.shape[0], 2])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(train):\n",
    "        dev_X, val_X = train_tfidf_a[dev_index], train_tfidf_a[val_index]\n",
    "        dev_y, val_y = train.loc[dev_index,col], train.loc[val_index,col]\n",
    "        pred_val_y, pred_test_y, model = mnb(dev_X, dev_y, val_X, val_y, test_tfidf_a)\n",
    "        pred_full_test_a = pred_full_test_a + pred_test_y\n",
    "        pred_train_a[val_index,:] = pred_val_y\n",
    "#         cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    \n",
    "    pred_full_test_a = pred_full_test_a / 5\n",
    "    train[\"nb_t_\"+col] = pred_train_t[:,0]\n",
    "    train[\"nb_t_\"+col] = pred_train_t[:,1]\n",
    "#     train_df[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "    test[\"nb_t_\"+col] = pred_full_test_t[:,0]\n",
    "    test[\"nb_t_\"+col] = pred_full_test_t[:,1]\n",
    "\n",
    "    train[\"nb_a_\"+col] = pred_train_a[:,0]\n",
    "    train[\"nb_a_\"+col] = pred_train_a[:,1]\n",
    "#     train_df[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "    test[\"nb_a_\"+col] = pred_full_test_a[:,0]\n",
    "    test[\"nb_a_\"+col] = pred_full_test_a[:,1]\n",
    "    \n",
    "    \n",
    "#     test_df[\"nb_cvec_mws\"] = pred_full_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec_t_2 = CountVectorizer(ngram_range=(1,3),analyzer='char')\n",
    "tfidf_vec_t_2.fit(train['TITLE'].values.tolist() + test['TITLE'].values.tolist())\n",
    "train_tfidf_t_2 = tfidf_vec_t_2.transform(train['TITLE'].values.tolist())\n",
    "test_tfidf_t_2 = tfidf_vec_t_2.transform(test['TITLE'].values.tolist())\n",
    "tfidf_vec_a_2 = CountVectorizer(ngram_range=(1,4),analyzer='char')\n",
    "tfidf_vec_a_2.fit(train['ABSTRACT'].values.tolist() + test['ABSTRACT'].values.tolist())\n",
    "train_tfidf_a_2 = tfidf_vec_a_2.transform(train['ABSTRACT'].values.tolist())\n",
    "test_tfidf_a_2 = tfidf_vec_a_2.transform(test['ABSTRACT'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']:\n",
    "    pred_full_test_t_2 = 0\n",
    "    pred_train_t_2 = np.zeros([train.shape[0], 2])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(train):\n",
    "        dev_X, val_X = train_tfidf_t_2[dev_index], train_tfidf_t_2[val_index]\n",
    "        dev_y, val_y = train.loc[dev_index,col], train.loc[val_index,col]\n",
    "        pred_val_y, pred_test_y, model = mnb(dev_X, dev_y, val_X, val_y, test_tfidf_t_2)\n",
    "        pred_full_test_t_2 = pred_full_test_t_2 + pred_test_y\n",
    "        pred_train_t_2[val_index,:] = pred_val_y\n",
    "#         cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    \n",
    "    pred_full_test_t_2 = pred_full_test_t_2 / 5\n",
    "    \n",
    "    pred_full_test_a_2 = 0\n",
    "    pred_train_a_2 = np.zeros([train.shape[0], 2])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(train):\n",
    "        dev_X, val_X = train_tfidf_a_2[dev_index], train_tfidf_a_2[val_index]\n",
    "        dev_y, val_y = train.loc[dev_index,col], train.loc[val_index,col]\n",
    "        pred_val_y, pred_test_y, model = mnb(dev_X, dev_y, val_X, val_y, test_tfidf_a_2)\n",
    "        pred_full_test_a_2 = pred_full_test_a_2 + pred_test_y\n",
    "        pred_train_a_2[val_index,:] = pred_val_y\n",
    "#         cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    \n",
    "    pred_full_test_a_2 = pred_full_test_a_2 / 5\n",
    "    train[\"nb_t_2\"+col] = pred_train_t_2[:,0]\n",
    "    train[\"nb_t_2\"+col] = pred_train_t_2[:,1]\n",
    "#     train_df[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "    test[\"nb_t_2\"+col] = pred_full_test_t_2[:,0]\n",
    "    test[\"nb_t_2\"+col] = pred_full_test_t_2[:,1]\n",
    "\n",
    "    train[\"nb_a_2\"+col] = pred_train_a_2[:,0]\n",
    "    train[\"nb_a_2\"+col] = pred_train_a_2[:,1]\n",
    "#     train_df[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "    test[\"nb_a_2\"+col] = pred_full_test_a_2[:,0]\n",
    "    test[\"nb_a_2\"+col] = pred_full_test_a_2[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec_t_2 = TfidfVectorizer(ngram_range=(1,4),analyzer='char')\n",
    "full_tfidf_t_2 = tfidf_vec_t_2.fit_transform(train['TITLE'].values.tolist() + test['TITLE'].values.tolist())\n",
    "train_tfidf_t_2 = tfidf_vec_t_2.transform(train['TITLE'].values.tolist())\n",
    "test_tfidf_t_2 = tfidf_vec_t_2.transform(test['TITLE'].values.tolist())\n",
    "tfidf_vec_a_2 = TfidfVectorizer(ngram_range=(1,5),analyzer='char')\n",
    "full_tfidf_a_2 = tfidf_vec_a_2.fit_transform(train['ABSTRACT'].values.tolist() + test['ABSTRACT'].values.tolist())\n",
    "train_tfidf_a_2 = tfidf_vec_a_2.transform(train['ABSTRACT'].values.tolist())\n",
    "test_tfidf_a_2 = tfidf_vec_a_2.transform(test['ABSTRACT'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t_2 = 10\n",
    "n_a_2 =25\n",
    "svd_t_2 = TruncatedSVD(n_components=n_t_2,algorithm='arpack')\n",
    "svd_a_2 = TruncatedSVD(n_components=n_a_2,algorithm='arpack')\n",
    "svd_t_2.fit(full_tfidf_t_2)\n",
    "svd_a_2.fit(full_tfidf_a_2)\n",
    "train_svd_t_2 = pd.DataFrame(svd_t_2.transform(train_tfidf_t_2))\n",
    "test_svd_t_2 = pd.DataFrame(svd_t_2.transform(test_tfidf_t_2))\n",
    "train_svd_a_2 = pd.DataFrame(svd_a_2.transform(train_tfidf_a_2))\n",
    "test_svd_a_2 = pd.DataFrame(svd_a_2.transform(test_tfidf_a_2))\n",
    "train_svd_t_2.columns = ['svd_word_t_2'+str(i) for i in range(n_t_2)]\n",
    "test_svd_t_2.columns = ['svd_word_t_2'+str(i) for i in range(n_t_2)]\n",
    "train = pd.concat([train, train_svd_t_2], axis=1)\n",
    "test = pd.concat([test, test_svd_t_2], axis=1)\n",
    "train_svd_a_2.columns = ['svd_word_a_2'+str(i) for i in range(n_a_2)]\n",
    "test_svd_a_2.columns = ['svd_word_a_2'+str(i) for i in range(n_a_2)]\n",
    "train = pd.concat([train, train_svd_a_2], axis=1)\n",
    "test = pd.concat([test, test_svd_a_2], axis=1)\n",
    "# del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']:\n",
    "#     pred_full_test_a = 0\n",
    "#     pred_train_a = np.zeros([train.shape[0], 2])\n",
    "#     kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "#     for dev_index, val_index in kf.split(train):\n",
    "#         dev_X, val_X = train_tfidf_a[dev_index], train_tfidf_a[val_index]\n",
    "#         dev_y, val_y = train.loc[dev_index,col], train.loc[val_index,col]\n",
    "#         pred_val_y, pred_test_y, model = mnb(dev_X, dev_y, val_X, val_y, test_tfidf_a)\n",
    "#         pred_full_test_a = pred_full_test_a + pred_test_y\n",
    "#         pred_train_a[val_index,:] = pred_val_y\n",
    "# #         cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    \n",
    "#     pred_full_test_a = pred_full_test_a / 5\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"nb_t_\"+col] = pred_train_t[:,0]\n",
    "# train[\"nb_t_\"+col] = pred_train_t[:,1]\n",
    "# #     train_df[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "# test[\"nb_t_\"+col] = pred_full_test_t[:,0]\n",
    "# test[\"nb_t_\"+col] = pred_full_test_t[:,1]\n",
    "\n",
    "# train[\"nb_a_\"+col] = pred_train_a[:,0]\n",
    "# train[\"nb_a_\"+col] = pred_train_a[:,1]\n",
    "# #     train_df[\"nb_cvec_mws\"] = pred_train[:,2]\n",
    "# test[\"nb_a_\"+col] = pred_full_test_a[:,0]\n",
    "# test[\"nb_a_\"+col] = pred_full_test_a[:,1]\n",
    "# #     test_df[\"nb_cvec_mws\"] = pred_full_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>...</th>\n",
       "      <th>nb_t_Physics</th>\n",
       "      <th>nb_a_Physics</th>\n",
       "      <th>nb_t_Mathematics</th>\n",
       "      <th>nb_a_Mathematics</th>\n",
       "      <th>nb_t_Statistics</th>\n",
       "      <th>nb_a_Statistics</th>\n",
       "      <th>nb_t_Quantitative Biology</th>\n",
       "      <th>nb_a_Quantitative Biology</th>\n",
       "      <th>nb_t_Quantitative Finance</th>\n",
       "      <th>nb_a_Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
       "      <td>Predictive models allow subject-specific inf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2.294661e-01</td>\n",
       "      <td>1.349981e-91</td>\n",
       "      <td>8.358065e-03</td>\n",
       "      <td>2.435926e-107</td>\n",
       "      <td>6.312107e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.077997e-04</td>\n",
       "      <td>1.102662e-135</td>\n",
       "      <td>2.203021e-05</td>\n",
       "      <td>3.927251e-252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rotation Invariance Neural Network</td>\n",
       "      <td>Rotation invariance and translation invarian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.649186e-03</td>\n",
       "      <td>4.582627e-34</td>\n",
       "      <td>8.272478e-06</td>\n",
       "      <td>3.486937e-43</td>\n",
       "      <td>6.875600e-02</td>\n",
       "      <td>7.153414e-01</td>\n",
       "      <td>6.324960e-06</td>\n",
       "      <td>9.184346e-55</td>\n",
       "      <td>6.403780e-09</td>\n",
       "      <td>1.147323e-83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
       "      <td>We introduce and develop the notion of spher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>2.352146e-03</td>\n",
       "      <td>8.651439e-15</td>\n",
       "      <td>9.994948e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.477615e-01</td>\n",
       "      <td>8.757600e-12</td>\n",
       "      <td>2.610139e-04</td>\n",
       "      <td>1.278738e-40</td>\n",
       "      <td>3.224106e-05</td>\n",
       "      <td>1.875691e-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A finite element approximation for the stochas...</td>\n",
       "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.344681e-02</td>\n",
       "      <td>1.842891e-09</td>\n",
       "      <td>9.997706e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.734992e-06</td>\n",
       "      <td>1.124711e-28</td>\n",
       "      <td>6.732965e-09</td>\n",
       "      <td>3.372085e-68</td>\n",
       "      <td>2.630909e-09</td>\n",
       "      <td>1.581201e-81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
       "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>5.028094e-07</td>\n",
       "      <td>1.064117e-37</td>\n",
       "      <td>2.159575e-06</td>\n",
       "      <td>1.851457e-52</td>\n",
       "      <td>9.992525e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.446716e-07</td>\n",
       "      <td>4.239707e-74</td>\n",
       "      <td>1.337440e-12</td>\n",
       "      <td>3.588775e-117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>On maximizing the fundamental frequency of the...</td>\n",
       "      <td>Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>7.430016e-03</td>\n",
       "      <td>4.026017e-50</td>\n",
       "      <td>8.325210e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.003247e-02</td>\n",
       "      <td>9.255756e-45</td>\n",
       "      <td>2.797581e-04</td>\n",
       "      <td>8.546183e-120</td>\n",
       "      <td>1.087253e-04</td>\n",
       "      <td>3.252726e-129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>On the rotation period and shape of the hyperb...</td>\n",
       "      <td>We observed the newly discovered hyperbolic ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>9.999917e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.725824e-03</td>\n",
       "      <td>1.059311e-25</td>\n",
       "      <td>2.744245e-04</td>\n",
       "      <td>1.316961e-23</td>\n",
       "      <td>5.015009e-05</td>\n",
       "      <td>1.597326e-38</td>\n",
       "      <td>3.751957e-05</td>\n",
       "      <td>1.787002e-53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Adverse effects of polymer coating on heat tra...</td>\n",
       "      <td>The ability of metallic nanoparticles to sup...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.506298e-06</td>\n",
       "      <td>1.269417e-50</td>\n",
       "      <td>8.112635e-07</td>\n",
       "      <td>1.677382e-45</td>\n",
       "      <td>2.571034e-07</td>\n",
       "      <td>9.258073e-61</td>\n",
       "      <td>5.843262e-09</td>\n",
       "      <td>7.981515e-95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SPH calculations of Mars-scale collisions: the...</td>\n",
       "      <td>We model large-scale ($\\approx$2000km) impac...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.820553e-05</td>\n",
       "      <td>2.028161e-43</td>\n",
       "      <td>1.592316e-06</td>\n",
       "      <td>2.683779e-40</td>\n",
       "      <td>3.017515e-07</td>\n",
       "      <td>2.018318e-68</td>\n",
       "      <td>3.841754e-09</td>\n",
       "      <td>2.195194e-99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>$\\mathcal{R}_{0}$ fails to predict the outbrea...</td>\n",
       "      <td>Time varying susceptibility of host at indiv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.869764e-03</td>\n",
       "      <td>4.911057e-07</td>\n",
       "      <td>5.550554e-02</td>\n",
       "      <td>1.051431e-22</td>\n",
       "      <td>9.390527e-01</td>\n",
       "      <td>2.868221e-23</td>\n",
       "      <td>1.338495e-03</td>\n",
       "      <td>4.050261e-55</td>\n",
       "      <td>1.096564e-05</td>\n",
       "      <td>3.571229e-93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>A global sensitivity analysis and reduced orde...</td>\n",
       "      <td>We present a systematic global sensitivity a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>7.813566e-04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.301911e-02</td>\n",
       "      <td>1.582333e-59</td>\n",
       "      <td>8.119632e-02</td>\n",
       "      <td>7.313705e-42</td>\n",
       "      <td>3.696729e-08</td>\n",
       "      <td>1.924475e-101</td>\n",
       "      <td>4.015521e-11</td>\n",
       "      <td>2.849943e-157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Role-separating ordering in social dilemmas co...</td>\n",
       "      <td>\"Three is a crowd\" is an old proverb that ap...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>6.265941e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.218011e-04</td>\n",
       "      <td>9.135989e-48</td>\n",
       "      <td>4.985500e-04</td>\n",
       "      <td>1.350167e-42</td>\n",
       "      <td>5.880836e-04</td>\n",
       "      <td>3.804877e-82</td>\n",
       "      <td>3.303036e-06</td>\n",
       "      <td>4.481831e-138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Dynamics of exciton magnetic polarons in CdMnS...</td>\n",
       "      <td>We study the exciton magnetic polaron (EMP) ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>9.999993e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.877205e-05</td>\n",
       "      <td>4.284347e-64</td>\n",
       "      <td>7.206909e-06</td>\n",
       "      <td>3.039411e-73</td>\n",
       "      <td>1.251877e-06</td>\n",
       "      <td>4.169319e-92</td>\n",
       "      <td>1.168852e-08</td>\n",
       "      <td>6.759064e-127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>On Varieties of Ordered Automata</td>\n",
       "      <td>The classical Eilenberg correspondence, base...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.337904e-03</td>\n",
       "      <td>5.096696e-36</td>\n",
       "      <td>9.885584e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.403430e-04</td>\n",
       "      <td>5.478292e-27</td>\n",
       "      <td>6.547508e-05</td>\n",
       "      <td>4.987864e-65</td>\n",
       "      <td>3.172587e-05</td>\n",
       "      <td>1.426545e-86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Direct Evidence of Spontaneous Abrikosov Vorte...</td>\n",
       "      <td>Using low-temperature Magnetic Force Microsc...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.973350e-09</td>\n",
       "      <td>1.953096e-68</td>\n",
       "      <td>6.962427e-11</td>\n",
       "      <td>4.209531e-97</td>\n",
       "      <td>2.960635e-09</td>\n",
       "      <td>1.881164e-107</td>\n",
       "      <td>8.790117e-10</td>\n",
       "      <td>1.930640e-145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>A rank 18 Waring decomposition of $sM_{\\langle...</td>\n",
       "      <td>The recent discovery that the exponent of ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1.288717e-01</td>\n",
       "      <td>1.662290e-11</td>\n",
       "      <td>9.997574e-01</td>\n",
       "      <td>9.999990e-01</td>\n",
       "      <td>9.553335e-01</td>\n",
       "      <td>2.183052e-10</td>\n",
       "      <td>6.931716e-01</td>\n",
       "      <td>5.861627e-29</td>\n",
       "      <td>3.147219e-01</td>\n",
       "      <td>9.856701e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>The PdBI Arcsecond Whirlpool Survey (PAWS). Th...</td>\n",
       "      <td>The process that leads to the formation of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>9.999997e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.297065e-05</td>\n",
       "      <td>3.755759e-139</td>\n",
       "      <td>1.286811e-04</td>\n",
       "      <td>1.686596e-128</td>\n",
       "      <td>5.438765e-05</td>\n",
       "      <td>5.958336e-160</td>\n",
       "      <td>8.399388e-06</td>\n",
       "      <td>8.604449e-216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Higher structure in the unstable Adams spectra...</td>\n",
       "      <td>We describe a variant construction of the un...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>2.779125e-02</td>\n",
       "      <td>4.338172e-14</td>\n",
       "      <td>8.756085e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.202997e-02</td>\n",
       "      <td>8.636339e-18</td>\n",
       "      <td>1.305437e-04</td>\n",
       "      <td>3.330596e-31</td>\n",
       "      <td>3.976702e-07</td>\n",
       "      <td>2.156268e-41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Comparing Covariate Prioritization via Matchin...</td>\n",
       "      <td>When investigators seek to estimate causal e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.988431e-13</td>\n",
       "      <td>1.234310e-66</td>\n",
       "      <td>2.040846e-09</td>\n",
       "      <td>4.470522e-61</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.463248e-14</td>\n",
       "      <td>1.434649e-103</td>\n",
       "      <td>2.566531e-17</td>\n",
       "      <td>5.659632e-153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Acoustic Impedance Calculation via Numerical S...</td>\n",
       "      <td>Assigning homogeneous boundary conditions, s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>9.467519e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.109556e-01</td>\n",
       "      <td>4.016049e-08</td>\n",
       "      <td>5.158586e-05</td>\n",
       "      <td>9.800127e-59</td>\n",
       "      <td>9.956826e-08</td>\n",
       "      <td>1.093972e-117</td>\n",
       "      <td>1.103880e-08</td>\n",
       "      <td>1.868245e-170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows  55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                              TITLE  \\\n",
       "0    1        Reconstructing Subject-Specific Effect Maps   \n",
       "1    2                 Rotation Invariance Neural Network   \n",
       "2    3  Spherical polyharmonics and Poisson kernels fo...   \n",
       "3    4  A finite element approximation for the stochas...   \n",
       "4    5  Comparative study of Discrete Wavelet Transfor...   \n",
       "5    6  On maximizing the fundamental frequency of the...   \n",
       "6    7  On the rotation period and shape of the hyperb...   \n",
       "7    8  Adverse effects of polymer coating on heat tra...   \n",
       "8    9  SPH calculations of Mars-scale collisions: the...   \n",
       "9   10  $\\mathcal{R}_{0}$ fails to predict the outbrea...   \n",
       "10  11  A global sensitivity analysis and reduced orde...   \n",
       "11  12  Role-separating ordering in social dilemmas co...   \n",
       "12  13  Dynamics of exciton magnetic polarons in CdMnS...   \n",
       "13  14                   On Varieties of Ordered Automata   \n",
       "14  15  Direct Evidence of Spontaneous Abrikosov Vorte...   \n",
       "15  16  A rank 18 Waring decomposition of $sM_{\\langle...   \n",
       "16  17  The PdBI Arcsecond Whirlpool Survey (PAWS). Th...   \n",
       "17  18  Higher structure in the unstable Adams spectra...   \n",
       "18  19  Comparing Covariate Prioritization via Matchin...   \n",
       "19  20  Acoustic Impedance Calculation via Numerical S...   \n",
       "\n",
       "                                             ABSTRACT  Computer Science  \\\n",
       "0     Predictive models allow subject-specific inf...                 1   \n",
       "1     Rotation invariance and translation invarian...                 1   \n",
       "2     We introduce and develop the notion of spher...                 0   \n",
       "3     The stochastic Landau--Lifshitz--Gilbert (LL...                 0   \n",
       "4     Fourier-transform infra-red (FTIR) spectra o...                 1   \n",
       "5     Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...                 0   \n",
       "6     We observed the newly discovered hyperbolic ...                 0   \n",
       "7     The ability of metallic nanoparticles to sup...                 0   \n",
       "8     We model large-scale ($\\approx$2000km) impac...                 0   \n",
       "9     Time varying susceptibility of host at indiv...                 0   \n",
       "10    We present a systematic global sensitivity a...                 1   \n",
       "11    \"Three is a crowd\" is an old proverb that ap...                 0   \n",
       "12    We study the exciton magnetic polaron (EMP) ...                 0   \n",
       "13    The classical Eilenberg correspondence, base...                 1   \n",
       "14    Using low-temperature Magnetic Force Microsc...                 0   \n",
       "15    The recent discovery that the exponent of ma...                 0   \n",
       "16    The process that leads to the formation of t...                 0   \n",
       "17    We describe a variant construction of the un...                 0   \n",
       "18    When investigators seek to estimate causal e...                 0   \n",
       "19    Assigning homogeneous boundary conditions, s...                 0   \n",
       "\n",
       "    Physics  Mathematics  Statistics  Quantitative Biology  \\\n",
       "0         0            0           0                     0   \n",
       "1         0            0           0                     0   \n",
       "2         0            1           0                     0   \n",
       "3         0            1           0                     0   \n",
       "4         0            0           1                     0   \n",
       "5         0            1           0                     0   \n",
       "6         1            0           0                     0   \n",
       "7         1            0           0                     0   \n",
       "8         1            0           0                     0   \n",
       "9         0            0           0                     1   \n",
       "10        0            0           0                     0   \n",
       "11        1            0           0                     0   \n",
       "12        1            0           0                     0   \n",
       "13        0            0           0                     0   \n",
       "14        1            0           0                     0   \n",
       "15        0            1           0                     0   \n",
       "16        1            0           0                     0   \n",
       "17        0            1           0                     0   \n",
       "18        0            0           1                     0   \n",
       "19        1            0           0                     0   \n",
       "\n",
       "    Quantitative Finance  num_words_title  ...  nb_t_Physics  nb_a_Physics  \\\n",
       "0                      0                4  ...  2.294661e-01  1.349981e-91   \n",
       "1                      0                4  ...  3.649186e-03  4.582627e-34   \n",
       "2                      0                8  ...  2.352146e-03  8.651439e-15   \n",
       "3                      0                9  ...  1.344681e-02  1.842891e-09   \n",
       "4                      0               17  ...  5.028094e-07  1.064117e-37   \n",
       "5                      0                9  ...  7.430016e-03  4.026017e-50   \n",
       "6                      0               15  ...  9.999917e-01  1.000000e+00   \n",
       "7                      0               11  ...  9.999999e-01  1.000000e+00   \n",
       "8                      0               14  ...  9.999996e-01  1.000000e+00   \n",
       "9                      0               12  ...  1.869764e-03  4.911057e-07   \n",
       "10                     0               12  ...  7.813566e-04  1.000000e+00   \n",
       "11                     0                9  ...  6.265941e-01  1.000000e+00   \n",
       "12                     0               12  ...  9.999993e-01  1.000000e+00   \n",
       "13                     0                5  ...  3.337904e-03  5.096696e-36   \n",
       "14                     0               13  ...  1.000000e+00  1.000000e+00   \n",
       "15                     0               11  ...  1.288717e-01  1.662290e-11   \n",
       "16                     0               15  ...  9.999997e-01  1.000000e+00   \n",
       "17                     0                8  ...  2.779125e-02  4.338172e-14   \n",
       "18                     0               16  ...  1.988431e-13  1.234310e-66   \n",
       "19                     0               11  ...  9.467519e-01  1.000000e+00   \n",
       "\n",
       "    nb_t_Mathematics  nb_a_Mathematics  nb_t_Statistics  nb_a_Statistics  \\\n",
       "0       8.358065e-03     2.435926e-107     6.312107e-02     1.000000e+00   \n",
       "1       8.272478e-06      3.486937e-43     6.875600e-02     7.153414e-01   \n",
       "2       9.994948e-01      1.000000e+00     3.477615e-01     8.757600e-12   \n",
       "3       9.997706e-01      1.000000e+00     4.734992e-06     1.124711e-28   \n",
       "4       2.159575e-06      1.851457e-52     9.992525e-01     1.000000e+00   \n",
       "5       8.325210e-01      1.000000e+00     5.003247e-02     9.255756e-45   \n",
       "6       2.725824e-03      1.059311e-25     2.744245e-04     1.316961e-23   \n",
       "7       1.506298e-06      1.269417e-50     8.112635e-07     1.677382e-45   \n",
       "8       2.820553e-05      2.028161e-43     1.592316e-06     2.683779e-40   \n",
       "9       5.550554e-02      1.051431e-22     9.390527e-01     2.868221e-23   \n",
       "10      4.301911e-02      1.582333e-59     8.119632e-02     7.313705e-42   \n",
       "11      4.218011e-04      9.135989e-48     4.985500e-04     1.350167e-42   \n",
       "12      9.877205e-05      4.284347e-64     7.206909e-06     3.039411e-73   \n",
       "13      9.885584e-01      1.000000e+00     5.403430e-04     5.478292e-27   \n",
       "14      1.973350e-09      1.953096e-68     6.962427e-11     4.209531e-97   \n",
       "15      9.997574e-01      9.999990e-01     9.553335e-01     2.183052e-10   \n",
       "16      1.297065e-05     3.755759e-139     1.286811e-04    1.686596e-128   \n",
       "17      8.756085e-01      1.000000e+00     2.202997e-02     8.636339e-18   \n",
       "18      2.040846e-09      4.470522e-61     9.999999e-01     1.000000e+00   \n",
       "19      2.109556e-01      4.016049e-08     5.158586e-05     9.800127e-59   \n",
       "\n",
       "    nb_t_Quantitative Biology  nb_a_Quantitative Biology  \\\n",
       "0                1.077997e-04              1.102662e-135   \n",
       "1                6.324960e-06               9.184346e-55   \n",
       "2                2.610139e-04               1.278738e-40   \n",
       "3                6.732965e-09               3.372085e-68   \n",
       "4                1.446716e-07               4.239707e-74   \n",
       "5                2.797581e-04              8.546183e-120   \n",
       "6                5.015009e-05               1.597326e-38   \n",
       "7                2.571034e-07               9.258073e-61   \n",
       "8                3.017515e-07               2.018318e-68   \n",
       "9                1.338495e-03               4.050261e-55   \n",
       "10               3.696729e-08              1.924475e-101   \n",
       "11               5.880836e-04               3.804877e-82   \n",
       "12               1.251877e-06               4.169319e-92   \n",
       "13               6.547508e-05               4.987864e-65   \n",
       "14               2.960635e-09              1.881164e-107   \n",
       "15               6.931716e-01               5.861627e-29   \n",
       "16               5.438765e-05              5.958336e-160   \n",
       "17               1.305437e-04               3.330596e-31   \n",
       "18               9.463248e-14              1.434649e-103   \n",
       "19               9.956826e-08              1.093972e-117   \n",
       "\n",
       "    nb_t_Quantitative Finance  nb_a_Quantitative Finance  \n",
       "0                2.203021e-05              3.927251e-252  \n",
       "1                6.403780e-09               1.147323e-83  \n",
       "2                3.224106e-05               1.875691e-50  \n",
       "3                2.630909e-09               1.581201e-81  \n",
       "4                1.337440e-12              3.588775e-117  \n",
       "5                1.087253e-04              3.252726e-129  \n",
       "6                3.751957e-05               1.787002e-53  \n",
       "7                5.843262e-09               7.981515e-95  \n",
       "8                3.841754e-09               2.195194e-99  \n",
       "9                1.096564e-05               3.571229e-93  \n",
       "10               4.015521e-11              2.849943e-157  \n",
       "11               3.303036e-06              4.481831e-138  \n",
       "12               1.168852e-08              6.759064e-127  \n",
       "13               3.172587e-05               1.426545e-86  \n",
       "14               8.790117e-10              1.930640e-145  \n",
       "15               3.147219e-01               9.856701e-37  \n",
       "16               8.399388e-06              8.604449e-216  \n",
       "17               3.976702e-07               2.156268e-41  \n",
       "18               2.566531e-17              5.659632e-153  \n",
       "19               1.103880e-08              1.868245e-170  \n",
       "\n",
       "[20 rows x 55 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'TITLE', 'ABSTRACT', 'Computer Science', 'Physics', 'Mathematics',\n",
       "       'Statistics', 'Quantitative Biology', 'Quantitative Finance',\n",
       "       'num_words_title', 'num_words_abs', 'word_upper_title',\n",
       "       'word_upper_abs', 'word_title_title', 'word_title_abs', 'svd_word_t0',\n",
       "       'svd_word_t1', 'svd_word_t2', 'svd_word_t3', 'svd_word_t4',\n",
       "       'svd_word_t5', 'svd_word_t6', 'svd_word_t7', 'svd_word_a0',\n",
       "       'svd_word_a1', 'svd_word_a2', 'svd_word_a3', 'svd_word_a4',\n",
       "       'svd_word_a5', 'svd_word_a6', 'svd_word_a7', 'svd_word_a8',\n",
       "       'svd_word_a9', 'svd_word_a10', 'svd_word_a11', 'svd_word_a12',\n",
       "       'svd_word_a13', 'svd_word_a14', 'svd_word_a15', 'svd_word_a16',\n",
       "       'svd_word_a17', 'svd_word_a18', 'svd_word_a19', 'nb_t_Computer Science',\n",
       "       'nb_a_Computer Science', 'nb_t_Physics', 'nb_a_Physics',\n",
       "       'nb_t_Mathematics', 'nb_a_Mathematics', 'nb_t_Statistics',\n",
       "       'nb_a_Statistics', 'nb_t_Quantitative Biology',\n",
       "       'nb_a_Quantitative Biology', 'nb_t_Quantitative Finance',\n",
       "       'nb_a_Quantitative Finance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.7):\n",
    "    param = {}\n",
    "    param['objective'] = 'binary:hinge'\n",
    "    param['eta'] = 0.05\n",
    "    param['max_depth'] = 14\n",
    "    param['silent'] = 1\n",
    "    param['eval_metric'] = \"logloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.6\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    param['n_jobs'] = -1\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:44:37] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:21.75071\ttest-logloss:21.71845\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:2.99746\ttest-logloss:4.76874\n",
      "[40]\ttrain-logloss:0.90473\ttest-logloss:4.44379\n",
      "[60]\ttrain-logloss:0.17348\ttest-logloss:4.50527\n",
      "[80]\ttrain-logloss:0.03733\ttest-logloss:4.50527\n",
      "Stopping. Best iteration:\n",
      "[30]\ttrain-logloss:1.73699\ttest-logloss:4.36475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:44:45] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:21.73973\ttest-logloss:21.76236\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:2.85253\ttest-logloss:5.63817\n",
      "[40]\ttrain-logloss:0.92010\ttest-logloss:4.99707\n",
      "[60]\ttrain-logloss:0.20203\ttest-logloss:4.77752\n",
      "[80]\ttrain-logloss:0.04392\ttest-logloss:5.02342\n",
      "[100]\ttrain-logloss:0.00659\ttest-logloss:5.04977\n",
      "Stopping. Best iteration:\n",
      "[60]\ttrain-logloss:0.20203\ttest-logloss:4.77752\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:44:56] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:21.74722\ttest-logloss:21.73242\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:2.76014\ttest-logloss:5.35842\n",
      "[40]\ttrain-logloss:0.81465\ttest-logloss:4.92799\n",
      "[60]\ttrain-logloss:0.18445\ttest-logloss:5.04218\n",
      "[80]\ttrain-logloss:0.02855\ttest-logloss:5.14760\n",
      "Stopping. Best iteration:\n",
      "[40]\ttrain-logloss:0.81465\ttest-logloss:4.92799\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:45:06] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:21.70111\ttest-logloss:21.91689\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:3.05437\ttest-logloss:5.11246\n",
      "[40]\ttrain-logloss:0.95079\ttest-logloss:4.27795\n",
      "[60]\ttrain-logloss:0.21958\ttest-logloss:4.40093\n",
      "[80]\ttrain-logloss:0.02196\ttest-logloss:4.55026\n",
      "Stopping. Best iteration:\n",
      "[39]\ttrain-logloss:1.02984\ttest-logloss:4.26917\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:45:17] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:21.78235\ttest-logloss:21.59187\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:2.86114\ttest-logloss:5.25301\n",
      "[40]\ttrain-logloss:0.87613\ttest-logloss:4.56783\n",
      "[60]\ttrain-logloss:0.17127\ttest-logloss:4.66446\n",
      "[80]\ttrain-logloss:0.01976\ttest-logloss:4.85771\n",
      "[100]\ttrain-logloss:0.00000\ttest-logloss:4.92799\n",
      "Stopping. Best iteration:\n",
      "[50]\ttrain-logloss:0.38207\ttest-logloss:4.44485\n",
      "\n",
      "cv scores :  [nan, nan, nan, nan, nan]\n",
      "[23:45:29] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:26.36200\ttest-logloss:25.94275\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.77956\ttest-logloss:2.71370\n",
      "[40]\ttrain-logloss:0.36672\ttest-logloss:2.30094\n",
      "[60]\ttrain-logloss:0.15591\ttest-logloss:2.29216\n",
      "[80]\ttrain-logloss:0.03733\ttest-logloss:2.27459\n",
      "[100]\ttrain-logloss:0.00000\ttest-logloss:2.30094\n",
      "[120]\ttrain-logloss:0.00000\ttest-logloss:2.30972\n",
      "Stopping. Best iteration:\n",
      "[79]\ttrain-logloss:0.03074\ttest-logloss:2.23068\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:45:42] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:26.25002\ttest-logloss:26.39064\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.81909\ttest-logloss:2.50293\n",
      "[40]\ttrain-logloss:0.38429\ttest-logloss:2.17799\n",
      "[60]\ttrain-logloss:0.13834\ttest-logloss:2.19555\n",
      "[80]\ttrain-logloss:0.03074\ttest-logloss:2.13408\n",
      "[100]\ttrain-logloss:0.00000\ttest-logloss:2.16042\n",
      "[120]\ttrain-logloss:0.00000\ttest-logloss:2.14286\n",
      "[140]\ttrain-logloss:0.00000\ttest-logloss:2.15164\n",
      "[160]\ttrain-logloss:0.00000\ttest-logloss:2.16920\n",
      "Stopping. Best iteration:\n",
      "[114]\ttrain-logloss:0.00000\ttest-logloss:2.08138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:45:57] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:26.25284\ttest-logloss:26.37937\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.81465\ttest-logloss:2.61772\n",
      "[40]\ttrain-logloss:0.40183\ttest-logloss:2.27513\n",
      "[60]\ttrain-logloss:0.14712\ttest-logloss:2.32784\n",
      "[80]\ttrain-logloss:0.03294\ttest-logloss:2.34541\n",
      "Stopping. Best iteration:\n",
      "[40]\ttrain-logloss:0.40183\ttest-logloss:2.27513\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:46:06] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:26.29676\ttest-logloss:26.20368\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.80806\ttest-logloss:2.69678\n",
      "[40]\ttrain-logloss:0.34913\ttest-logloss:2.20486\n",
      "[60]\ttrain-logloss:0.11638\ttest-logloss:2.17851\n",
      "[80]\ttrain-logloss:0.03074\ttest-logloss:2.16972\n",
      "[100]\ttrain-logloss:0.00220\ttest-logloss:2.19607\n",
      "[120]\ttrain-logloss:0.00000\ttest-logloss:2.24000\n",
      "Stopping. Best iteration:\n",
      "[79]\ttrain-logloss:0.03074\ttest-logloss:2.12580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:46:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:26.22869\ttest-logloss:26.47599\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.83221\ttest-logloss:2.84611\n",
      "[40]\ttrain-logloss:0.38427\ttest-logloss:2.40690\n",
      "[60]\ttrain-logloss:0.13834\ttest-logloss:2.45960\n",
      "[80]\ttrain-logloss:0.03513\ttest-logloss:2.40690\n",
      "Stopping. Best iteration:\n",
      "[38]\ttrain-logloss:0.41062\ttest-logloss:2.38055\n",
      "\n",
      "cv scores :  [nan, nan, nan, nan, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:46:28] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:26.97465\ttest-logloss:26.96149\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.03209\ttest-logloss:4.15398\n",
      "[40]\ttrain-logloss:0.30304\ttest-logloss:3.28454\n",
      "[60]\ttrain-logloss:0.12078\ttest-logloss:3.34602\n",
      "[80]\ttrain-logloss:0.02196\ttest-logloss:3.31967\n",
      "Stopping. Best iteration:\n",
      "[45]\ttrain-logloss:0.25912\ttest-logloss:3.23185\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:46:39] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:26.89560\ttest-logloss:27.27766\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.14189\ttest-logloss:4.30328\n",
      "[40]\ttrain-logloss:0.28986\ttest-logloss:3.34602\n",
      "[60]\ttrain-logloss:0.09443\ttest-logloss:3.34602\n",
      "[80]\ttrain-logloss:0.01757\ttest-logloss:3.42506\n",
      "Stopping. Best iteration:\n",
      "[47]\ttrain-logloss:0.19764\ttest-logloss:3.31967\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:46:50] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:26.94450\ttest-logloss:27.08212\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.02984\ttest-logloss:4.41850\n",
      "[40]\ttrain-logloss:0.36451\ttest-logloss:3.68940\n",
      "[60]\ttrain-logloss:0.10540\ttest-logloss:3.64548\n",
      "[80]\ttrain-logloss:0.02415\ttest-logloss:3.71576\n",
      "[100]\ttrain-logloss:0.00220\ttest-logloss:3.56643\n",
      "Stopping. Best iteration:\n",
      "[66]\ttrain-logloss:0.06587\ttest-logloss:3.52250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:47:02] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:27.01695\ttest-logloss:26.79223\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.03862\ttest-logloss:4.11105\n",
      "[40]\ttrain-logloss:0.31400\ttest-logloss:3.43466\n",
      "[60]\ttrain-logloss:0.07905\ttest-logloss:3.51372\n",
      "[80]\ttrain-logloss:0.02635\ttest-logloss:3.46101\n",
      "[100]\ttrain-logloss:0.00000\ttest-logloss:3.45223\n",
      "Stopping. Best iteration:\n",
      "[50]\ttrain-logloss:0.16249\ttest-logloss:3.39074\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:47:14] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:27.02793\ttest-logloss:26.74831\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.13743\ttest-logloss:4.47121\n",
      "[40]\ttrain-logloss:0.33815\ttest-logloss:3.48737\n",
      "[60]\ttrain-logloss:0.12955\ttest-logloss:3.48737\n",
      "[80]\ttrain-logloss:0.01976\ttest-logloss:3.48737\n",
      "Stopping. Best iteration:\n",
      "[45]\ttrain-logloss:0.27009\ttest-logloss:3.36439\n",
      "\n",
      "cv scores :  [nan, nan, nan, nan, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:47:25] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:27.55435\ttest-logloss:28.26128\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.84240\ttest-logloss:4.63700\n",
      "[40]\ttrain-logloss:0.37331\ttest-logloss:3.89930\n",
      "[60]\ttrain-logloss:0.12078\ttest-logloss:3.74122\n",
      "[80]\ttrain-logloss:0.03294\ttest-logloss:3.62705\n",
      "[100]\ttrain-logloss:0.01098\ttest-logloss:3.84660\n",
      "[120]\ttrain-logloss:0.00000\ttest-logloss:3.89052\n",
      "Stopping. Best iteration:\n",
      "[76]\ttrain-logloss:0.04392\ttest-logloss:3.59192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:47:38] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:27.80907\ttest-logloss:27.24253\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.76554\ttest-logloss:4.80386\n",
      "[40]\ttrain-logloss:0.38649\ttest-logloss:4.32963\n",
      "[60]\ttrain-logloss:0.13615\ttest-logloss:4.17155\n",
      "[80]\ttrain-logloss:0.03074\ttest-logloss:4.22424\n",
      "[100]\ttrain-logloss:0.00220\ttest-logloss:4.26815\n",
      "Stopping. Best iteration:\n",
      "[61]\ttrain-logloss:0.13176\ttest-logloss:4.10129\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:47:50] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:27.79644\ttest-logloss:27.29295\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.81374\ttest-logloss:4.69960\n",
      "[40]\ttrain-logloss:0.45014\ttest-logloss:4.07591\n",
      "[60]\ttrain-logloss:0.14273\ttest-logloss:4.18133\n",
      "[80]\ttrain-logloss:0.03294\ttest-logloss:4.22525\n",
      "Stopping. Best iteration:\n",
      "[42]\ttrain-logloss:0.39525\ttest-logloss:3.99686\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:48:00] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:27.62297\ttest-logloss:27.98691\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.83131\ttest-logloss:4.76987\n",
      "[40]\ttrain-logloss:0.45453\ttest-logloss:3.98807\n",
      "[60]\ttrain-logloss:0.16908\ttest-logloss:4.06713\n",
      "[80]\ttrain-logloss:0.04831\ttest-logloss:4.15497\n",
      "[100]\ttrain-logloss:0.00659\ttest-logloss:4.15497\n",
      "Stopping. Best iteration:\n",
      "[51]\ttrain-logloss:0.23715\ttest-logloss:3.80360\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:48:11] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:27.69543\ttest-logloss:27.69703\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:1.83131\ttest-logloss:4.94556\n",
      "[40]\ttrain-logloss:0.45453\ttest-logloss:4.10227\n",
      "[60]\ttrain-logloss:0.15151\ttest-logloss:4.15497\n",
      "[80]\ttrain-logloss:0.02855\ttest-logloss:4.19889\n",
      "Stopping. Best iteration:\n",
      "[46]\ttrain-logloss:0.33596\ttest-logloss:4.05835\n",
      "\n",
      "cv scores :  [nan, nan, nan, nan, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:48:22] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:35.80027\ttest-logloss:35.84919\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.31402\ttest-logloss:0.87822\n",
      "[40]\ttrain-logloss:0.34257\ttest-logloss:0.83431\n",
      "[60]\ttrain-logloss:0.11199\ttest-logloss:0.84309\n",
      "Stopping. Best iteration:\n",
      "[23]\ttrain-logloss:0.36233\ttest-logloss:0.79918\n",
      "\n",
      "[23:48:29] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:35.82662\ttest-logloss:35.74380\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.26132\ttest-logloss:1.07143\n",
      "[40]\ttrain-logloss:0.28328\ttest-logloss:1.01874\n",
      "[60]\ttrain-logloss:0.10321\ttest-logloss:0.98361\n",
      "[80]\ttrain-logloss:0.01757\ttest-logloss:1.05387\n",
      "[100]\ttrain-logloss:0.00000\ttest-logloss:1.08899\n",
      "Stopping. Best iteration:\n",
      "[59]\ttrain-logloss:0.11199\ttest-logloss:0.97483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:48:38] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:35.80472\ttest-logloss:35.83139\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.32498\ttest-logloss:1.08047\n",
      "[40]\ttrain-logloss:0.30961\ttest-logloss:0.99263\n",
      "[60]\ttrain-logloss:0.09881\ttest-logloss:0.99263\n",
      "[80]\ttrain-logloss:0.00439\ttest-logloss:1.01898\n",
      "Stopping. Best iteration:\n",
      "[46]\ttrain-logloss:0.22837\ttest-logloss:0.95749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:48:46] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:35.80253\ttest-logloss:35.84017\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.27887\ttest-logloss:1.00141\n",
      "[40]\ttrain-logloss:0.29644\ttest-logloss:0.92235\n",
      "[60]\ttrain-logloss:0.09442\ttest-logloss:0.89600\n",
      "[80]\ttrain-logloss:0.01317\ttest-logloss:0.87843\n",
      "[100]\ttrain-logloss:0.00000\ttest-logloss:0.87843\n",
      "Stopping. Best iteration:\n",
      "[68]\ttrain-logloss:0.05489\ttest-logloss:0.85208\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:48:57] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:35.81570\ttest-logloss:35.78746\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.27667\ttest-logloss:1.05412\n",
      "[40]\ttrain-logloss:0.31839\ttest-logloss:0.95749\n",
      "[60]\ttrain-logloss:0.11418\ttest-logloss:1.01019\n",
      "[80]\ttrain-logloss:0.01537\ttest-logloss:1.04533\n",
      "Stopping. Best iteration:\n",
      "[40]\ttrain-logloss:0.31839\ttest-logloss:0.95749\n",
      "\n",
      "cv scores :  [nan, nan, nan, nan, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:49:09] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:36.41298\ttest-logloss:36.36734\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.15591\ttest-logloss:0.38642\n",
      "[40]\ttrain-logloss:0.16250\ttest-logloss:0.42155\n",
      "[60]\ttrain-logloss:0.06588\ttest-logloss:0.41276\n",
      "[80]\ttrain-logloss:0.01537\ttest-logloss:0.38642\n",
      "[100]\ttrain-logloss:0.00000\ttest-logloss:0.37763\n",
      "[120]\ttrain-logloss:0.00000\ttest-logloss:0.38642\n",
      "Stopping. Best iteration:\n",
      "[75]\ttrain-logloss:0.02196\ttest-logloss:0.36885\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:49:20] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:36.40639\ttest-logloss:36.39370\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.15591\ttest-logloss:0.41276\n",
      "[40]\ttrain-logloss:0.16030\ttest-logloss:0.41276\n",
      "[60]\ttrain-logloss:0.06807\ttest-logloss:0.39520\n",
      "[80]\ttrain-logloss:0.01757\ttest-logloss:0.40398\n",
      "[100]\ttrain-logloss:0.00439\ttest-logloss:0.40398\n",
      "Stopping. Best iteration:\n",
      "[64]\ttrain-logloss:0.06149\ttest-logloss:0.37763\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:49:30] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:36.39983\ttest-logloss:36.41994\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.18445\ttest-logloss:0.42165\n",
      "[40]\ttrain-logloss:0.18445\ttest-logloss:0.36016\n",
      "[60]\ttrain-logloss:0.07905\ttest-logloss:0.36894\n",
      "[80]\ttrain-logloss:0.00659\ttest-logloss:0.36016\n",
      "[100]\ttrain-logloss:0.00000\ttest-logloss:0.36016\n",
      "[120]\ttrain-logloss:0.00000\ttest-logloss:0.36016\n",
      "Stopping. Best iteration:\n",
      "[70]\ttrain-logloss:0.03513\ttest-logloss:0.32502\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:49:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:36.39543\ttest-logloss:36.43751\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.18445\ttest-logloss:0.38651\n",
      "[40]\ttrain-logloss:0.18664\ttest-logloss:0.38651\n",
      "[60]\ttrain-logloss:0.09222\ttest-logloss:0.36894\n",
      "[80]\ttrain-logloss:0.02196\ttest-logloss:0.36894\n",
      "[100]\ttrain-logloss:0.00439\ttest-logloss:0.36894\n",
      "[120]\ttrain-logloss:0.00220\ttest-logloss:0.36894\n",
      "[140]\ttrain-logloss:0.00000\ttest-logloss:0.35137\n",
      "Stopping. Best iteration:\n",
      "[94]\ttrain-logloss:0.00878\ttest-logloss:0.34259\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:49:54] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:36.40422\ttest-logloss:36.40237\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-logloss:0.17347\ttest-logloss:0.38651\n",
      "[40]\ttrain-logloss:0.15371\ttest-logloss:0.34259\n",
      "[60]\ttrain-logloss:0.07027\ttest-logloss:0.35137\n",
      "[80]\ttrain-logloss:0.00878\ttest-logloss:0.36016\n",
      "[100]\ttrain-logloss:0.00000\ttest-logloss:0.34259\n",
      "[120]\ttrain-logloss:0.00000\ttest-logloss:0.34259\n",
      "Stopping. Best iteration:\n",
      "[70]\ttrain-logloss:0.03733\ttest-logloss:0.33380\n",
      "\n",
      "cv scores :  [nan, nan, nan, nan, nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "C:\\Users\\120638\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2240: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "train_x = train.drop(columns=['ID','TITLE','ABSTRACT','Computer Science', 'Physics', 'Mathematics',\n",
    "       'Statistics', 'Quantitative Biology', 'Quantitative Finance'])\n",
    "preds = pd.DataFrame(np.zeros(test.shape[0]))\n",
    "pred_train_xg = pd.DataFrame(np.zeros(train.shape[0]))\n",
    "test_x = test.drop(columns=['ID','TITLE','ABSTRACT'])\n",
    "for col in ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']:\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train_xgb = np.zeros([train.shape[0]])\n",
    "    for dev_index, val_index in kf.split(train):\n",
    "        dev_X, val_X = train_x.loc[dev_index], train_x.loc[val_index]\n",
    "        dev_y, val_y = train.loc[dev_index,col], train.loc[val_index,col]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_x, seed_val=0)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train_xgb[val_index] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    preds[col] = pred_full_test//3\n",
    "    pred_train_xg[col] = pred_train_xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>num_words_abs</th>\n",
       "      <th>num_unique_title</th>\n",
       "      <th>num_unique_abs</th>\n",
       "      <th>word_upper_title</th>\n",
       "      <th>word_upper_abs</th>\n",
       "      <th>word_title_title</th>\n",
       "      <th>...</th>\n",
       "      <th>svd_word_a_215</th>\n",
       "      <th>svd_word_a_216</th>\n",
       "      <th>svd_word_a_217</th>\n",
       "      <th>svd_word_a_218</th>\n",
       "      <th>svd_word_a_219</th>\n",
       "      <th>svd_word_a_220</th>\n",
       "      <th>svd_word_a_221</th>\n",
       "      <th>svd_word_a_222</th>\n",
       "      <th>svd_word_a_223</th>\n",
       "      <th>svd_word_a_224</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
       "      <td>We present novel understandings of the Gamma...</td>\n",
       "      <td>7</td>\n",
       "      <td>96</td>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029011</td>\n",
       "      <td>-0.019336</td>\n",
       "      <td>-0.015417</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>-0.026391</td>\n",
       "      <td>-0.028771</td>\n",
       "      <td>0.017370</td>\n",
       "      <td>-0.062069</td>\n",
       "      <td>-0.044792</td>\n",
       "      <td>-0.012903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
       "      <td>Meteorites contain minerals from Solar Syste...</td>\n",
       "      <td>14</td>\n",
       "      <td>135</td>\n",
       "      <td>13</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>0.010092</td>\n",
       "      <td>0.024814</td>\n",
       "      <td>0.007854</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>Case For Static AMSDU Aggregation in WLANs</td>\n",
       "      <td>Frame aggregation is a mechanism by which mu...</td>\n",
       "      <td>7</td>\n",
       "      <td>126</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>-0.034381</td>\n",
       "      <td>-0.020763</td>\n",
       "      <td>-0.025617</td>\n",
       "      <td>-0.001453</td>\n",
       "      <td>-0.017915</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>0.014997</td>\n",
       "      <td>-0.033013</td>\n",
       "      <td>0.013496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
       "      <td>Milky Way open clusters are very diverse in ...</td>\n",
       "      <td>11</td>\n",
       "      <td>268</td>\n",
       "      <td>11</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.034634</td>\n",
       "      <td>-0.008034</td>\n",
       "      <td>-0.022068</td>\n",
       "      <td>0.023631</td>\n",
       "      <td>-0.008512</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>-0.030776</td>\n",
       "      <td>0.023139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
       "      <td>Proving that a cryptographic protocol is cor...</td>\n",
       "      <td>11</td>\n",
       "      <td>119</td>\n",
       "      <td>11</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008500</td>\n",
       "      <td>0.016019</td>\n",
       "      <td>-0.023167</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.019665</td>\n",
       "      <td>-0.009444</td>\n",
       "      <td>-0.048115</td>\n",
       "      <td>-0.045064</td>\n",
       "      <td>-0.010259</td>\n",
       "      <td>-0.044421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              TITLE  \\\n",
       "0  20973  Closed-form Marginal Likelihood in Gamma-Poiss...   \n",
       "1  20974  Laboratory mid-IR spectra of equilibrated and ...   \n",
       "2  20975         Case For Static AMSDU Aggregation in WLANs   \n",
       "3  20976  The $Gaia$-ESO Survey: the inner disk intermed...   \n",
       "4  20977  Witness-Functions versus Interpretation-Functi...   \n",
       "\n",
       "                                            ABSTRACT  num_words_title  \\\n",
       "0    We present novel understandings of the Gamma...                7   \n",
       "1    Meteorites contain minerals from Solar Syste...               14   \n",
       "2    Frame aggregation is a mechanism by which mu...                7   \n",
       "3    Milky Way open clusters are very diverse in ...               11   \n",
       "4    Proving that a cryptographic protocol is cor...               11   \n",
       "\n",
       "   num_words_abs  num_unique_title  num_unique_abs  word_upper_title  \\\n",
       "0             96                 7              75                 0   \n",
       "1            135                13              96                 0   \n",
       "2            126                 7              85                 1   \n",
       "3            268                11             164                 1   \n",
       "4            119                11              78                 0   \n",
       "\n",
       "   word_upper_abs  word_title_title  ...  svd_word_a_215  svd_word_a_216  \\\n",
       "0               0                 5  ...        0.029011       -0.019336   \n",
       "1               0                 2  ...        0.014684        0.006344   \n",
       "2               7                 4  ...        0.029525       -0.034381   \n",
       "3              13                 2  ...        0.034751        0.028613   \n",
       "4               0                 7  ...       -0.008500        0.016019   \n",
       "\n",
       "   svd_word_a_217  svd_word_a_218  svd_word_a_219  svd_word_a_220  \\\n",
       "0       -0.015417        0.011438       -0.026391       -0.028771   \n",
       "1        0.000462        0.034874       -0.008300        0.010092   \n",
       "2       -0.020763       -0.025617       -0.001453       -0.017915   \n",
       "3        0.034634       -0.008034       -0.022068        0.023631   \n",
       "4       -0.023167        0.006702        0.019665       -0.009444   \n",
       "\n",
       "   svd_word_a_221  svd_word_a_222  svd_word_a_223  svd_word_a_224  \n",
       "0        0.017370       -0.062069       -0.044792       -0.012903  \n",
       "1        0.024814        0.007854       -0.000325        0.000300  \n",
       "2        0.008269        0.014997       -0.033013        0.013496  \n",
       "3       -0.008512        0.008492       -0.030776        0.023139  \n",
       "4       -0.048115       -0.045064       -0.010259       -0.044421  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Computer Science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  Computer Science\n",
       "0   0.0               0.0\n",
       "1   0.0               0.0\n",
       "2   0.0               1.0\n",
       "3   0.0               0.0\n",
       "4   0.0               1.0\n",
       "5   0.0               0.0\n",
       "6   0.0               0.0\n",
       "7   0.0               1.0\n",
       "8   0.0               1.0\n",
       "9   0.0               0.0\n",
       "10  0.0               0.0\n",
       "11  0.0               1.0\n",
       "12  0.0               0.0\n",
       "13  0.0               1.0\n",
       "14  0.0               1.0\n",
       "15  0.0               1.0\n",
       "16  0.0               0.0\n",
       "17  0.0               1.0\n",
       "18  0.0               1.0\n",
       "19  0.0               1.0\n",
       "20  0.0               0.0\n",
       "21  0.0               0.0\n",
       "22  0.0               1.0\n",
       "23  0.0               1.0\n",
       "24  0.0               0.0\n",
       "25  0.0               1.0\n",
       "26  0.0               1.0\n",
       "27  0.0               0.0\n",
       "28  0.0               0.0\n",
       "29  0.0               1.0"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.drop(columns=[0],inplace=True)\n",
    "need = test[['ID']]\n",
    "preds_final2 = pd.concat([need,preds],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Computer Science  Physics  Mathematics  Statistics  \\\n",
       "0  20973               0.0      0.0          0.0         1.0   \n",
       "1  20974               0.0      1.0          0.0         0.0   \n",
       "2  20975               1.0      0.0          0.0         0.0   \n",
       "3  20976               0.0      1.0          0.0         0.0   \n",
       "4  20977               1.0      0.0          0.0         0.0   \n",
       "\n",
       "   Quantitative Biology  Quantitative Finance  \n",
       "0                   0.0                   0.0  \n",
       "1                   0.0                   0.0  \n",
       "2                   0.0                   0.0  \n",
       "3                   0.0                   0.0  \n",
       "4                   0.0                   0.0  "
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_final2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8989, 109)\n",
      "(8989, 6)\n",
      "(8989, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "print(preds.shape)\n",
    "print(need.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID\n",
       "0  20973\n",
       "1  20974\n",
       "2  20975\n",
       "3  20976\n",
       "4  20977"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub.shape\n",
    "need.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_final2.set_index('ID',inplace=True)\n",
    "preds_final2.to_csv('srk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Computer Science</th>\n",
       "      <th>Physics</th>\n",
       "      <th>Mathematics</th>\n",
       "      <th>Statistics</th>\n",
       "      <th>Quantitative Biology</th>\n",
       "      <th>Quantitative Finance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20973</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20974</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20975</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20976</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20977</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Computer Science  Physics  Mathematics  Statistics  \\\n",
       "ID                                                          \n",
       "20973               0.0      0.0          0.0         1.0   \n",
       "20974               0.0      1.0          0.0         0.0   \n",
       "20975               1.0      0.0          0.0         0.0   \n",
       "20976               0.0      1.0          0.0         0.0   \n",
       "20977               1.0      0.0          0.0         0.0   \n",
       "\n",
       "       Quantitative Biology  Quantitative Finance  \n",
       "ID                                                 \n",
       "20973                   0.0                   0.0  \n",
       "20974                   0.0                   0.0  \n",
       "20975                   0.0                   0.0  \n",
       "20976                   0.0                   0.0  \n",
       "20977                   0.0                   0.0  "
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_final2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.7):\n",
    "    param = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': {'binary_logloss'},\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.6,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'bagging_freq' : 5,\n",
    "            'max_depth' : 12,\n",
    "            'min_child_weight' : 1 ,\n",
    "            'n_jobs' : -1\n",
    "            }\n",
    " \n",
    "    \n",
    "    lgb_train = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgb_eval = lgb.Dataset(test_X, label=test_y)\n",
    "#         watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = lgb.train(param,\n",
    "                lgb_train,\n",
    "                num_boost_round=2000,\n",
    "                valid_sets=(lgb_train, lgb_eval),\n",
    "               early_stopping_rounds=20,\n",
    "               verbose_eval = 20)\n",
    "        \n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "    \n",
    "    print(test_x.shape)\n",
    "    print(test_X2.shape)\n",
    "\n",
    "    pred_test_y = model.predict(test_X, ntree_limit = model.best_iteration)\n",
    "    if test_X2 is not None:\n",
    "        lgbtest2 = test_X2\n",
    "        pred_test_y2 = model.predict(lgbtest2, ntree_limit = model.best_iteration)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.376895\tvalid_1's binary_logloss: 0.383051\n",
      "[40]\ttraining's binary_logloss: 0.294351\tvalid_1's binary_logloss: 0.310271\n",
      "[60]\ttraining's binary_logloss: 0.259807\tvalid_1's binary_logloss: 0.287444\n",
      "[80]\ttraining's binary_logloss: 0.237332\tvalid_1's binary_logloss: 0.277902\n",
      "[100]\ttraining's binary_logloss: 0.220581\tvalid_1's binary_logloss: 0.274589\n",
      "[120]\ttraining's binary_logloss: 0.206899\tvalid_1's binary_logloss: 0.272151\n",
      "[140]\ttraining's binary_logloss: 0.19454\tvalid_1's binary_logloss: 0.272694\n",
      "Early stopping, best iteration is:\n",
      "[130]\ttraining's binary_logloss: 0.200381\tvalid_1's binary_logloss: 0.271937\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.37392\tvalid_1's binary_logloss: 0.393192\n",
      "[40]\ttraining's binary_logloss: 0.289813\tvalid_1's binary_logloss: 0.325784\n",
      "[60]\ttraining's binary_logloss: 0.254784\tvalid_1's binary_logloss: 0.306087\n",
      "[80]\ttraining's binary_logloss: 0.232964\tvalid_1's binary_logloss: 0.298403\n",
      "[100]\ttraining's binary_logloss: 0.216773\tvalid_1's binary_logloss: 0.295898\n",
      "[120]\ttraining's binary_logloss: 0.202904\tvalid_1's binary_logloss: 0.293797\n",
      "[140]\ttraining's binary_logloss: 0.190697\tvalid_1's binary_logloss: 0.293913\n",
      "Early stopping, best iteration is:\n",
      "[123]\ttraining's binary_logloss: 0.201087\tvalid_1's binary_logloss: 0.293614\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.373426\tvalid_1's binary_logloss: 0.391832\n",
      "[40]\ttraining's binary_logloss: 0.289959\tvalid_1's binary_logloss: 0.324026\n",
      "[60]\ttraining's binary_logloss: 0.254639\tvalid_1's binary_logloss: 0.305301\n",
      "[80]\ttraining's binary_logloss: 0.232803\tvalid_1's binary_logloss: 0.298722\n",
      "[100]\ttraining's binary_logloss: 0.216417\tvalid_1's binary_logloss: 0.297121\n",
      "[120]\ttraining's binary_logloss: 0.202351\tvalid_1's binary_logloss: 0.296917\n",
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's binary_logloss: 0.205033\tvalid_1's binary_logloss: 0.296511\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.377466\tvalid_1's binary_logloss: 0.381942\n",
      "[40]\ttraining's binary_logloss: 0.295141\tvalid_1's binary_logloss: 0.30704\n",
      "[60]\ttraining's binary_logloss: 0.26028\tvalid_1's binary_logloss: 0.282612\n",
      "[80]\ttraining's binary_logloss: 0.238723\tvalid_1's binary_logloss: 0.27349\n",
      "[100]\ttraining's binary_logloss: 0.222794\tvalid_1's binary_logloss: 0.269925\n",
      "[120]\ttraining's binary_logloss: 0.209296\tvalid_1's binary_logloss: 0.266706\n",
      "[140]\ttraining's binary_logloss: 0.197373\tvalid_1's binary_logloss: 0.266044\n",
      "[160]\ttraining's binary_logloss: 0.185668\tvalid_1's binary_logloss: 0.265376\n",
      "[180]\ttraining's binary_logloss: 0.175433\tvalid_1's binary_logloss: 0.26552\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttraining's binary_logloss: 0.182309\tvalid_1's binary_logloss: 0.265326\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.373677\tvalid_1's binary_logloss: 0.39398\n",
      "[40]\ttraining's binary_logloss: 0.290455\tvalid_1's binary_logloss: 0.325452\n",
      "[60]\ttraining's binary_logloss: 0.255673\tvalid_1's binary_logloss: 0.306063\n",
      "[80]\ttraining's binary_logloss: 0.233906\tvalid_1's binary_logloss: 0.299076\n",
      "[100]\ttraining's binary_logloss: 0.217351\tvalid_1's binary_logloss: 0.295317\n",
      "[120]\ttraining's binary_logloss: 0.20338\tvalid_1's binary_logloss: 0.294667\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's binary_logloss: 0.210381\tvalid_1's binary_logloss: 0.294174\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "cv scores :  [0.2719369550252391, 0.29361423990684166, 0.2965107602134078, 0.2653259212736124, 0.2941741986482968]\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.260217\tvalid_1's binary_logloss: 0.277157\n",
      "[40]\ttraining's binary_logloss: 0.171942\tvalid_1's binary_logloss: 0.199748\n",
      "[60]\ttraining's binary_logloss: 0.135329\tvalid_1's binary_logloss: 0.175917\n",
      "[80]\ttraining's binary_logloss: 0.1145\tvalid_1's binary_logloss: 0.168353\n",
      "[100]\ttraining's binary_logloss: 0.0996898\tvalid_1's binary_logloss: 0.165245\n",
      "[120]\ttraining's binary_logloss: 0.0884739\tvalid_1's binary_logloss: 0.164565\n",
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's binary_logloss: 0.0903939\tvalid_1's binary_logloss: 0.164339\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.262886\tvalid_1's binary_logloss: 0.270604\n",
      "[40]\ttraining's binary_logloss: 0.1736\tvalid_1's binary_logloss: 0.190902\n",
      "[60]\ttraining's binary_logloss: 0.137812\tvalid_1's binary_logloss: 0.166647\n",
      "[80]\ttraining's binary_logloss: 0.116821\tvalid_1's binary_logloss: 0.158076\n",
      "[100]\ttraining's binary_logloss: 0.10215\tvalid_1's binary_logloss: 0.154647\n",
      "[120]\ttraining's binary_logloss: 0.0900379\tvalid_1's binary_logloss: 0.153488\n",
      "[140]\ttraining's binary_logloss: 0.0804261\tvalid_1's binary_logloss: 0.152509\n",
      "[160]\ttraining's binary_logloss: 0.0719407\tvalid_1's binary_logloss: 0.15178\n",
      "[180]\ttraining's binary_logloss: 0.0644314\tvalid_1's binary_logloss: 0.151551\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's binary_logloss: 0.0651582\tvalid_1's binary_logloss: 0.151466\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.2603\tvalid_1's binary_logloss: 0.278224\n",
      "[40]\ttraining's binary_logloss: 0.171392\tvalid_1's binary_logloss: 0.202398\n",
      "[60]\ttraining's binary_logloss: 0.134788\tvalid_1's binary_logloss: 0.178717\n",
      "[80]\ttraining's binary_logloss: 0.114074\tvalid_1's binary_logloss: 0.170792\n",
      "[100]\ttraining's binary_logloss: 0.0992934\tvalid_1's binary_logloss: 0.168295\n",
      "[120]\ttraining's binary_logloss: 0.0875677\tvalid_1's binary_logloss: 0.167912\n",
      "[140]\ttraining's binary_logloss: 0.0778647\tvalid_1's binary_logloss: 0.168251\n",
      "Early stopping, best iteration is:\n",
      "[126]\ttraining's binary_logloss: 0.0844963\tvalid_1's binary_logloss: 0.167253\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.262385\tvalid_1's binary_logloss: 0.271347\n",
      "[40]\ttraining's binary_logloss: 0.173018\tvalid_1's binary_logloss: 0.192513\n",
      "[60]\ttraining's binary_logloss: 0.136972\tvalid_1's binary_logloss: 0.168748\n",
      "[80]\ttraining's binary_logloss: 0.116035\tvalid_1's binary_logloss: 0.160062\n",
      "[100]\ttraining's binary_logloss: 0.101377\tvalid_1's binary_logloss: 0.156233\n",
      "[120]\ttraining's binary_logloss: 0.0899722\tvalid_1's binary_logloss: 0.154863\n",
      "[140]\ttraining's binary_logloss: 0.0805312\tvalid_1's binary_logloss: 0.154353\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's binary_logloss: 0.0823686\tvalid_1's binary_logloss: 0.154155\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.260023\tvalid_1's binary_logloss: 0.277709\n",
      "[40]\ttraining's binary_logloss: 0.170352\tvalid_1's binary_logloss: 0.202888\n",
      "[60]\ttraining's binary_logloss: 0.133961\tvalid_1's binary_logloss: 0.180051\n",
      "[80]\ttraining's binary_logloss: 0.113452\tvalid_1's binary_logloss: 0.171886\n",
      "[100]\ttraining's binary_logloss: 0.0987265\tvalid_1's binary_logloss: 0.169389\n",
      "[120]\ttraining's binary_logloss: 0.0871375\tvalid_1's binary_logloss: 0.169275\n",
      "Early stopping, best iteration is:\n",
      "[114]\ttraining's binary_logloss: 0.0905334\tvalid_1's binary_logloss: 0.168992\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "cv scores :  [0.1643390626381342, 0.1514660426584914, 0.16725265189640492, 0.15415523073444118, 0.1689923731000664]\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.291028\tvalid_1's binary_logloss: 0.306357\n",
      "[40]\ttraining's binary_logloss: 0.214054\tvalid_1's binary_logloss: 0.242655\n",
      "[60]\ttraining's binary_logloss: 0.181323\tvalid_1's binary_logloss: 0.223481\n",
      "[80]\ttraining's binary_logloss: 0.161156\tvalid_1's binary_logloss: 0.216413\n",
      "[100]\ttraining's binary_logloss: 0.145825\tvalid_1's binary_logloss: 0.212441\n",
      "[120]\ttraining's binary_logloss: 0.133473\tvalid_1's binary_logloss: 0.210702\n",
      "[140]\ttraining's binary_logloss: 0.122799\tvalid_1's binary_logloss: 0.210557\n",
      "Early stopping, best iteration is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134]\ttraining's binary_logloss: 0.125791\tvalid_1's binary_logloss: 0.210158\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.293278\tvalid_1's binary_logloss: 0.304092\n",
      "[40]\ttraining's binary_logloss: 0.215673\tvalid_1's binary_logloss: 0.240865\n",
      "[60]\ttraining's binary_logloss: 0.182136\tvalid_1's binary_logloss: 0.221268\n",
      "[80]\ttraining's binary_logloss: 0.161786\tvalid_1's binary_logloss: 0.215063\n",
      "[100]\ttraining's binary_logloss: 0.146526\tvalid_1's binary_logloss: 0.213297\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's binary_logloss: 0.147891\tvalid_1's binary_logloss: 0.213098\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.290638\tvalid_1's binary_logloss: 0.310559\n",
      "[40]\ttraining's binary_logloss: 0.213033\tvalid_1's binary_logloss: 0.248612\n",
      "[60]\ttraining's binary_logloss: 0.179349\tvalid_1's binary_logloss: 0.231081\n",
      "[80]\ttraining's binary_logloss: 0.159191\tvalid_1's binary_logloss: 0.224414\n",
      "[100]\ttraining's binary_logloss: 0.144114\tvalid_1's binary_logloss: 0.221375\n",
      "[120]\ttraining's binary_logloss: 0.131616\tvalid_1's binary_logloss: 0.220494\n",
      "[140]\ttraining's binary_logloss: 0.120914\tvalid_1's binary_logloss: 0.220037\n",
      "Early stopping, best iteration is:\n",
      "[131]\ttraining's binary_logloss: 0.125544\tvalid_1's binary_logloss: 0.219915\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.292085\tvalid_1's binary_logloss: 0.306174\n",
      "[40]\ttraining's binary_logloss: 0.214935\tvalid_1's binary_logloss: 0.241773\n",
      "[60]\ttraining's binary_logloss: 0.181837\tvalid_1's binary_logloss: 0.221778\n",
      "[80]\ttraining's binary_logloss: 0.161759\tvalid_1's binary_logloss: 0.215134\n",
      "[100]\ttraining's binary_logloss: 0.146957\tvalid_1's binary_logloss: 0.212444\n",
      "[120]\ttraining's binary_logloss: 0.134516\tvalid_1's binary_logloss: 0.210832\n",
      "[140]\ttraining's binary_logloss: 0.123941\tvalid_1's binary_logloss: 0.210095\n",
      "[160]\ttraining's binary_logloss: 0.114562\tvalid_1's binary_logloss: 0.211117\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's binary_logloss: 0.123941\tvalid_1's binary_logloss: 0.210095\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.29173\tvalid_1's binary_logloss: 0.307879\n",
      "[40]\ttraining's binary_logloss: 0.215194\tvalid_1's binary_logloss: 0.243688\n",
      "[60]\ttraining's binary_logloss: 0.181355\tvalid_1's binary_logloss: 0.222956\n",
      "[80]\ttraining's binary_logloss: 0.160863\tvalid_1's binary_logloss: 0.215362\n",
      "[100]\ttraining's binary_logloss: 0.146034\tvalid_1's binary_logloss: 0.21214\n",
      "[120]\ttraining's binary_logloss: 0.133364\tvalid_1's binary_logloss: 0.211707\n",
      "[140]\ttraining's binary_logloss: 0.122768\tvalid_1's binary_logloss: 0.211498\n",
      "[160]\ttraining's binary_logloss: 0.113205\tvalid_1's binary_logloss: 0.21147\n",
      "[180]\ttraining's binary_logloss: 0.104279\tvalid_1's binary_logloss: 0.211872\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's binary_logloss: 0.108443\tvalid_1's binary_logloss: 0.211393\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "cv scores :  [0.21015814828115273, 0.21309771642057862, 0.2199154771379402, 0.2100954081927639, 0.21139252886281515]\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.306208\tvalid_1's binary_logloss: 0.305633\n",
      "[40]\ttraining's binary_logloss: 0.237326\tvalid_1's binary_logloss: 0.251599\n",
      "[60]\ttraining's binary_logloss: 0.207715\tvalid_1's binary_logloss: 0.23657\n",
      "[80]\ttraining's binary_logloss: 0.188716\tvalid_1's binary_logloss: 0.230873\n",
      "[100]\ttraining's binary_logloss: 0.174274\tvalid_1's binary_logloss: 0.228142\n",
      "[120]\ttraining's binary_logloss: 0.16159\tvalid_1's binary_logloss: 0.227714\n",
      "[140]\ttraining's binary_logloss: 0.150262\tvalid_1's binary_logloss: 0.226505\n",
      "[160]\ttraining's binary_logloss: 0.140264\tvalid_1's binary_logloss: 0.226003\n",
      "[180]\ttraining's binary_logloss: 0.131119\tvalid_1's binary_logloss: 0.22588\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's binary_logloss: 0.132425\tvalid_1's binary_logloss: 0.225673\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.299637\tvalid_1's binary_logloss: 0.329215\n",
      "[40]\ttraining's binary_logloss: 0.232277\tvalid_1's binary_logloss: 0.274222\n",
      "[60]\ttraining's binary_logloss: 0.202501\tvalid_1's binary_logloss: 0.257555\n",
      "[80]\ttraining's binary_logloss: 0.18329\tvalid_1's binary_logloss: 0.252303\n",
      "[100]\ttraining's binary_logloss: 0.168797\tvalid_1's binary_logloss: 0.250583\n",
      "[120]\ttraining's binary_logloss: 0.156412\tvalid_1's binary_logloss: 0.250219\n",
      "[140]\ttraining's binary_logloss: 0.145608\tvalid_1's binary_logloss: 0.250312\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's binary_logloss: 0.154128\tvalid_1's binary_logloss: 0.249867\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.299734\tvalid_1's binary_logloss: 0.327505\n",
      "[40]\ttraining's binary_logloss: 0.232725\tvalid_1's binary_logloss: 0.272501\n",
      "[60]\ttraining's binary_logloss: 0.203125\tvalid_1's binary_logloss: 0.255022\n",
      "[80]\ttraining's binary_logloss: 0.184424\tvalid_1's binary_logloss: 0.248677\n",
      "[100]\ttraining's binary_logloss: 0.169769\tvalid_1's binary_logloss: 0.246746\n",
      "[120]\ttraining's binary_logloss: 0.15712\tvalid_1's binary_logloss: 0.246011\n",
      "[140]\ttraining's binary_logloss: 0.145987\tvalid_1's binary_logloss: 0.245232\n",
      "[160]\ttraining's binary_logloss: 0.135976\tvalid_1's binary_logloss: 0.245979\n",
      "Early stopping, best iteration is:\n",
      "[144]\ttraining's binary_logloss: 0.144018\tvalid_1's binary_logloss: 0.245087\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.304923\tvalid_1's binary_logloss: 0.313669\n",
      "[40]\ttraining's binary_logloss: 0.236005\tvalid_1's binary_logloss: 0.258718\n",
      "[60]\ttraining's binary_logloss: 0.205962\tvalid_1's binary_logloss: 0.242374\n",
      "[80]\ttraining's binary_logloss: 0.186908\tvalid_1's binary_logloss: 0.236924\n",
      "[100]\ttraining's binary_logloss: 0.172172\tvalid_1's binary_logloss: 0.234444\n",
      "[120]\ttraining's binary_logloss: 0.159952\tvalid_1's binary_logloss: 0.23331\n",
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's binary_logloss: 0.162295\tvalid_1's binary_logloss: 0.233298\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.30266\tvalid_1's binary_logloss: 0.316976\n",
      "[40]\ttraining's binary_logloss: 0.234866\tvalid_1's binary_logloss: 0.26295\n",
      "[60]\ttraining's binary_logloss: 0.20522\tvalid_1's binary_logloss: 0.245986\n",
      "[80]\ttraining's binary_logloss: 0.185967\tvalid_1's binary_logloss: 0.241032\n",
      "[100]\ttraining's binary_logloss: 0.171299\tvalid_1's binary_logloss: 0.240156\n",
      "[120]\ttraining's binary_logloss: 0.159071\tvalid_1's binary_logloss: 0.238956\n",
      "[140]\ttraining's binary_logloss: 0.148154\tvalid_1's binary_logloss: 0.238777\n",
      "[160]\ttraining's binary_logloss: 0.138216\tvalid_1's binary_logloss: 0.238008\n",
      "Early stopping, best iteration is:\n",
      "[156]\ttraining's binary_logloss: 0.140063\tvalid_1's binary_logloss: 0.237822\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "cv scores :  [0.22567295532082743, 0.24986744894856047, 0.2450867050368448, 0.2332979295976922, 0.23782229928671558]\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0682802\tvalid_1's binary_logloss: 0.087433\n",
      "[40]\ttraining's binary_logloss: 0.0475327\tvalid_1's binary_logloss: 0.0797872\n",
      "[60]\ttraining's binary_logloss: 0.035621\tvalid_1's binary_logloss: 0.0776334\n",
      "[80]\ttraining's binary_logloss: 0.0274749\tvalid_1's binary_logloss: 0.0766527\n",
      "[100]\ttraining's binary_logloss: 0.0215038\tvalid_1's binary_logloss: 0.0765759\n",
      "[120]\ttraining's binary_logloss: 0.0171516\tvalid_1's binary_logloss: 0.077128\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttraining's binary_logloss: 0.0210174\tvalid_1's binary_logloss: 0.0763845\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0654126\tvalid_1's binary_logloss: 0.101388\n",
      "[40]\ttraining's binary_logloss: 0.0456395\tvalid_1's binary_logloss: 0.0943201\n",
      "[60]\ttraining's binary_logloss: 0.0342917\tvalid_1's binary_logloss: 0.0919943\n",
      "[80]\ttraining's binary_logloss: 0.0265996\tvalid_1's binary_logloss: 0.0917036\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttraining's binary_logloss: 0.029787\tvalid_1's binary_logloss: 0.0913051\n",
      "(8989, 106)\n",
      "(8989, 106)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0673573\tvalid_1's binary_logloss: 0.0917869\n",
      "[40]\ttraining's binary_logloss: 0.0467235\tvalid_1's binary_logloss: 0.0837475\n",
      "[60]\ttraining's binary_logloss: 0.0351966\tvalid_1's binary_logloss: 0.0809596\n",
      "[80]\ttraining's binary_logloss: 0.0272177\tvalid_1's binary_logloss: 0.0806443\n",
      "Early stopping, best iteration is:\n",
      "[75]\ttraining's binary_logloss: 0.0290078\tvalid_1's binary_logloss: 0.0801908\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0684449\tvalid_1's binary_logloss: 0.090831\n",
      "[40]\ttraining's binary_logloss: 0.0481947\tvalid_1's binary_logloss: 0.0831117\n",
      "[60]\ttraining's binary_logloss: 0.0363344\tvalid_1's binary_logloss: 0.0804505\n",
      "[80]\ttraining's binary_logloss: 0.0282094\tvalid_1's binary_logloss: 0.0800484\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's binary_logloss: 0.0311617\tvalid_1's binary_logloss: 0.0798002\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0663923\tvalid_1's binary_logloss: 0.0919343\n",
      "[40]\ttraining's binary_logloss: 0.0473172\tvalid_1's binary_logloss: 0.0842597\n",
      "[60]\ttraining's binary_logloss: 0.0356259\tvalid_1's binary_logloss: 0.0817176\n",
      "[80]\ttraining's binary_logloss: 0.0275504\tvalid_1's binary_logloss: 0.0811559\n",
      "[100]\ttraining's binary_logloss: 0.0215814\tvalid_1's binary_logloss: 0.0810942\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttraining's binary_logloss: 0.02261\tvalid_1's binary_logloss: 0.0809312\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "cv scores :  [0.07638450100559423, 0.09130512495326493, 0.08019083214449454, 0.07980016214563283, 0.08093116887798418]\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.025718\tvalid_1's binary_logloss: 0.0454787\n",
      "[40]\ttraining's binary_logloss: 0.0141253\tvalid_1's binary_logloss: 0.0416508\n",
      "[60]\ttraining's binary_logloss: 0.00865034\tvalid_1's binary_logloss: 0.0407501\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's binary_logloss: 0.00949078\tvalid_1's binary_logloss: 0.0406202\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0248717\tvalid_1's binary_logloss: 0.0475683\n",
      "[40]\ttraining's binary_logloss: 0.0140756\tvalid_1's binary_logloss: 0.0434645\n",
      "[60]\ttraining's binary_logloss: 0.00853416\tvalid_1's binary_logloss: 0.0428074\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's binary_logloss: 0.00937003\tvalid_1's binary_logloss: 0.0427235\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0249748\tvalid_1's binary_logloss: 0.0424378\n",
      "[40]\ttraining's binary_logloss: 0.0141771\tvalid_1's binary_logloss: 0.0378433\n",
      "[60]\ttraining's binary_logloss: 0.00843663\tvalid_1's binary_logloss: 0.0368149\n",
      "[80]\ttraining's binary_logloss: 0.00547268\tvalid_1's binary_logloss: 0.0369132\n",
      "Early stopping, best iteration is:\n",
      "[62]\ttraining's binary_logloss: 0.00799727\tvalid_1's binary_logloss: 0.0366433\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0262958\tvalid_1's binary_logloss: 0.0387121\n",
      "[40]\ttraining's binary_logloss: 0.0143755\tvalid_1's binary_logloss: 0.0343691\n",
      "[60]\ttraining's binary_logloss: 0.0089174\tvalid_1's binary_logloss: 0.0333606\n",
      "[80]\ttraining's binary_logloss: 0.00586586\tvalid_1's binary_logloss: 0.0330331\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's binary_logloss: 0.00687348\tvalid_1's binary_logloss: 0.0329009\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\ttraining's binary_logloss: 0.026004\tvalid_1's binary_logloss: 0.040581\n",
      "[40]\ttraining's binary_logloss: 0.0144805\tvalid_1's binary_logloss: 0.0365048\n",
      "[60]\ttraining's binary_logloss: 0.00923703\tvalid_1's binary_logloss: 0.0347457\n",
      "[80]\ttraining's binary_logloss: 0.00606485\tvalid_1's binary_logloss: 0.0345008\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttraining's binary_logloss: 0.0072798\tvalid_1's binary_logloss: 0.0343812\n",
      "(8989, 106)\n",
      "(8989, 106)\n",
      "cv scores :  [0.04062018404876805, 0.042723492668426094, 0.036643290716057736, 0.032900940391685035, 0.03438118060023734]\n"
     ]
    }
   ],
   "source": [
    "train_x = train.drop(columns=['ID','TITLE','ABSTRACT','Computer Science', 'Physics', 'Mathematics',\n",
    "       'Statistics', 'Quantitative Biology', 'Quantitative Finance'])\n",
    "preds_lg = pd.DataFrame(np.zeros(test.shape[0]))\n",
    "pred_train_l = pd.DataFrame(np.zeros(train.shape[0]))\n",
    "test_ = test.drop(columns=['ID','TITLE','ABSTRACT'])\n",
    "for col in ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']:\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train_lg = np.zeros([train.shape[0]])\n",
    "    for dev_index, val_index in kf.split(train):\n",
    "        dev_X, val_X = train_x.loc[dev_index], train_x.loc[val_index]\n",
    "        dev_y, val_y = train.loc[dev_index,col], train.loc[val_index,col]\n",
    "        pred_val_y, pred_test_y, model = runLGB(dev_X, dev_y, val_X, val_y, test_, seed_val=0)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train_lg[val_index] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    preds_lg[col] = pred_full_test//3\n",
    "    pred_train_l[col] = pred_train_lg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_lg.drop(columns=[0],inplace=True)\n",
    "need = test[['ID']]\n",
    "preds_final2_lg = pd.concat([need,preds_lg],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_final2_lg.set_index('ID',inplace=True)\n",
    "preds_final2_lg.to_csv('srk_lg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runCB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.7):\n",
    "    param = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary:hinge',\n",
    "            'metric': {'binary_logloss'},\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.7,\n",
    "            'bagging_fraction': 0.6,\n",
    "            'bagging_freq' : 5,\n",
    "            'max_depth' : 10,\n",
    "            'min_child_weight' : 1 \n",
    "            }\n",
    " \n",
    "    \n",
    "    lgb_train = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        model = CatBoostClassifier(iterations=500,\n",
    "                             learning_rate=0.05,\n",
    "                             depth=10,\n",
    "                             eval_metric='Logloss',\n",
    "                             random_seed = 42,\n",
    "                             bagging_temperature = 0.2,\n",
    "                             od_type='Iter',\n",
    "                             metric_period = 50,\n",
    "                             od_wait=20)\n",
    "        model.fit(train_X,train_y,eval_set=(test_X,test_y),use_best_model=True,verbose=50)\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    if test_X2 is not None:\n",
    "        lgbtest2 = test_X2\n",
    "        pred_test_y2 = model.predict(lgbtest2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6469417\ttest: 0.6475127\tbest: 0.6475127 (0)\ttotal: 436ms\tremaining: 3m 37s\n",
      "50:\tlearn: 0.2425304\ttest: 0.2921967\tbest: 0.2921967 (50)\ttotal: 22.8s\tremaining: 3m 20s\n",
      "100:\tlearn: 0.1897402\ttest: 0.2818803\tbest: 0.2818803 (100)\ttotal: 44.6s\tremaining: 2m 56s\n",
      "150:\tlearn: 0.1491950\ttest: 0.2795479\tbest: 0.2793050 (147)\ttotal: 1m 6s\tremaining: 2m 32s\n",
      "200:\tlearn: 0.1201979\ttest: 0.2794450\tbest: 0.2786520 (183)\ttotal: 1m 28s\tremaining: 2m 11s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.2786519677\n",
      "bestIteration = 183\n",
      "\n",
      "Shrink model to first 184 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6463561\ttest: 0.6493867\tbest: 0.6493867 (0)\ttotal: 437ms\tremaining: 3m 38s\n",
      "50:\tlearn: 0.2365874\ttest: 0.3123554\tbest: 0.3123554 (50)\ttotal: 22.7s\tremaining: 3m 19s\n",
      "100:\tlearn: 0.1836050\ttest: 0.3039380\tbest: 0.3039380 (100)\ttotal: 44.7s\tremaining: 2m 56s\n",
      "150:\tlearn: 0.1475733\ttest: 0.3023238\tbest: 0.3021931 (143)\ttotal: 1m 7s\tremaining: 2m 34s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.3018145363\n",
      "bestIteration = 167\n",
      "\n",
      "Shrink model to first 168 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6485288\ttest: 0.6506632\tbest: 0.6506632 (0)\ttotal: 433ms\tremaining: 3m 35s\n",
      "50:\tlearn: 0.2370344\ttest: 0.3090650\tbest: 0.3090650 (50)\ttotal: 22.6s\tremaining: 3m 18s\n",
      "100:\tlearn: 0.1845596\ttest: 0.3044139\tbest: 0.3044139 (100)\ttotal: 44.1s\tremaining: 2m 54s\n",
      "150:\tlearn: 0.1461442\ttest: 0.3034377\tbest: 0.3033201 (141)\ttotal: 1m 5s\tremaining: 2m 32s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.303150798\n",
      "bestIteration = 154\n",
      "\n",
      "Shrink model to first 155 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6475451\ttest: 0.6481587\tbest: 0.6481587 (0)\ttotal: 462ms\tremaining: 3m 50s\n",
      "50:\tlearn: 0.2446659\ttest: 0.2870280\tbest: 0.2870280 (50)\ttotal: 23.2s\tremaining: 3m 24s\n",
      "100:\tlearn: 0.1916911\ttest: 0.2746485\tbest: 0.2746485 (100)\ttotal: 44.9s\tremaining: 2m 57s\n",
      "150:\tlearn: 0.1533378\ttest: 0.2704675\tbest: 0.2704675 (150)\ttotal: 1m 7s\tremaining: 2m 36s\n",
      "200:\tlearn: 0.1276952\ttest: 0.2693811\tbest: 0.2693811 (200)\ttotal: 1m 31s\tremaining: 2m 15s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.2687546716\n",
      "bestIteration = 219\n",
      "\n",
      "Shrink model to first 220 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6467003\ttest: 0.6484844\tbest: 0.6484844 (0)\ttotal: 588ms\tremaining: 4m 53s\n",
      "50:\tlearn: 0.2385191\ttest: 0.3057885\tbest: 0.3057885 (50)\ttotal: 22.7s\tremaining: 3m 20s\n",
      "100:\tlearn: 0.1860881\ttest: 0.2985312\tbest: 0.2985148 (98)\ttotal: 45.3s\tremaining: 2m 59s\n",
      "150:\tlearn: 0.1500219\ttest: 0.2967548\tbest: 0.2966876 (149)\ttotal: 1m 8s\tremaining: 2m 37s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.2966514458\n",
      "bestIteration = 151\n",
      "\n",
      "Shrink model to first 152 iterations.\n",
      "cv scores :  [4.124953431856543, 4.495466544797175, 4.51300956294856, 3.9529952656069036, 4.430644913690149]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6259459\ttest: 0.6276147\tbest: 0.6276147 (0)\ttotal: 457ms\tremaining: 3m 48s\n",
      "50:\tlearn: 0.1279789\ttest: 0.1695624\tbest: 0.1695624 (50)\ttotal: 22.5s\tremaining: 3m 17s\n",
      "100:\tlearn: 0.0899026\ttest: 0.1619207\tbest: 0.1618967 (97)\ttotal: 44s\tremaining: 2m 53s\n",
      "150:\tlearn: 0.0663918\ttest: 0.1606665\tbest: 0.1606665 (150)\ttotal: 1m 5s\tremaining: 2m 31s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.1606466135\n",
      "bestIteration = 152\n",
      "\n",
      "Shrink model to first 153 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6266727\ttest: 0.6265981\tbest: 0.6265981 (0)\ttotal: 424ms\tremaining: 3m 31s\n",
      "50:\tlearn: 0.1292994\ttest: 0.1610201\tbest: 0.1610201 (50)\ttotal: 21.9s\tremaining: 3m 12s\n",
      "100:\tlearn: 0.0926176\ttest: 0.1542161\tbest: 0.1542161 (100)\ttotal: 44.1s\tremaining: 2m 54s\n",
      "150:\tlearn: 0.0697602\ttest: 0.1521791\tbest: 0.1521467 (149)\ttotal: 1m 6s\tremaining: 2m 33s\n",
      "200:\tlearn: 0.0534481\ttest: 0.1509669\tbest: 0.1509532 (198)\ttotal: 1m 29s\tremaining: 2m 12s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.1507261618\n",
      "bestIteration = 224\n",
      "\n",
      "Shrink model to first 225 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Overfitting detector is active, thus evaluation metric is calculated on every iteration. 'metric_period' is ignored for evaluation metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6250858\ttest: 0.6278066\tbest: 0.6278066 (0)\ttotal: 472ms\tremaining: 3m 55s\n",
      "50:\tlearn: 0.1264327\ttest: 0.1755952\tbest: 0.1755952 (50)\ttotal: 22.4s\tremaining: 3m 17s\n",
      "100:\tlearn: 0.0881040\ttest: 0.1692414\tbest: 0.1691872 (99)\ttotal: 45.8s\tremaining: 3m 1s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-410-1759a6ef084a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdev_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdev_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mdev_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdev_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mpred_val_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_test_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunCB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mpred_full_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_full_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpred_test_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpred_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_val_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-409-d06e03a96ff8>\u001b[0m in \u001b[0;36mrunCB\u001b[1;34m(train_X, train_y, test_X, test_y, test_X2, seed_val, child, colsample)\u001b[0m\n\u001b[0;32m     25\u001b[0m                              \u001b[0mmetric_period\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                              od_wait=20)\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muse_best_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[0;32m   4113\u001b[0m         self._fit(X, y, cat_features, text_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0;32m   4114\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[0;32m   4116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[0;32m   1741\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1742\u001b[0m                 \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m                 \u001b[0mtrain_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"init_model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m             )\n\u001b[0;32m   1745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1230\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1231\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x = train.drop(columns=['ID','TITLE','ABSTRACT','Computer Science', 'Physics', 'Mathematics',\n",
    "       'Statistics', 'Quantitative Biology', 'Quantitative Finance'])\n",
    "preds_cb = pd.DataFrame(np.zeros(test.shape[0]))\n",
    "preds_train_c = pd.DataFrame(np.zeros(train.shape[0]))\n",
    "test__ = test.drop(columns=['ID','TITLE','ABSTRACT'])\n",
    "for col in ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']:\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([train.shape[0]])\n",
    "    for dev_index, val_index in kf.split(train):\n",
    "        dev_X, val_X = train_x.loc[dev_index], train_x.loc[val_index]\n",
    "        dev_y, val_y = train.loc[dev_index,col], train.loc[val_index,col]\n",
    "        pred_val_y, pred_test_y, model = runCB(dev_X, dev_y, val_X, val_y, test__, seed_val=0)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    preds_cb[col] = pred_full_test//3\n",
    "    preds_train_c = pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRF(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.7):\n",
    "    param = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "            \n",
    " \n",
    "    \n",
    "#     lgb_train = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        model = RandomForestClassifier('n_jobs': -1,\n",
    "    'n_estimators': 50,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0)\n",
    "        model.fit(train_X,train_y)\n",
    "    \n",
    "        \n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    if test_X2 is not None:\n",
    "        lgbtest2 = test_X2\n",
    "        pred_test_y2 = model.predict(lgbtest2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.drop(columns=['ID','TITLE','ABSTRACT','Computer Science', 'Physics', 'Mathematics',\n",
    "       'Statistics', 'Quantitative Biology', 'Quantitative Finance'])\n",
    "preds_rf = pd.DataFrame(np.zeros(test.shape[0]))\n",
    "test___ = test.drop(columns=['ID','TITLE','ABSTRACT'])\n",
    "for col in ['Computer Science','Physics','Mathematics','Statistics','Quantitative Biology','Quantitative Finance']:\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train_rf = np.zeros([train.shape[0]])\n",
    "    for dev_index, val_index in kf.split(train):\n",
    "        dev_X, val_X = train_x.loc[dev_index], train_x.loc[val_index]\n",
    "        dev_y, val_y = train.loc[dev_index,col], train.loc[val_index,col]\n",
    "        pred_val_y, pred_test_y, model = runCB(dev_X, dev_y, val_X, val_y, test___, seed_val=0)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train_rf[val_index] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    preds_rf[col] = pred_full_test//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cs = pd.concat([pred_train_cb[:,'Computer Science'],pred_train_xgb[:,1],pred_train_lg[:,1],pred_train_rf[:,1]],axis=1)\n",
    "train_phy = pd.concat([pred_train_cb[:,'Computer Science'],pred_train_xgb[:,2],pred_train_lg[:,2],pred_train_rf[:,2]],axis=1)\n",
    "train_mat = pd.concat([pred_train_cb[:,'Computer Science'],pred_train_xgb[:,3],pred_train_lg[:,3],pred_train_rf[:,3]],axis=1)\n",
    "train_sta = pd.concat([pred_train_cb[:,'Computer Science'],pred_train_xgb[:,4],pred_train_lg[:,4],pred_train_rf[:,4]],axis=1)\n",
    "train_qb = pd.concat([pred_train_cb[:,'Computer Science'],pred_train_xgb[:,5],pred_train_lg[:,5],pred_train_rf[:,5]],axis=1)\n",
    "train_qf = pd.concat([pred_train_cb[:,6],pred_train_xgb[:,6],pred_train_lg[:,6],pred_train_rf[:,6]],axis=1)\n",
    "\n",
    "test_cs = pd.concat([preds_cb[:,'Computer Science'],preds[:,'Computer Science'],preds_lg[:,'Computer Science'],preds_rf[:,'Computer Science']],axis=1)\n",
    "test_phy = pd.concat([preds_cb[:,'Physics'],preds[:,'Physics'],preds_lg[:,'Physics'],preds_rf[:,'Physics']],axis=1)\n",
    "test_mat = pd.concat([preds_cb[:,'Mathematics'],preds[:,'Mathematics'],preds_lg[:,'Mathematics'],preds_rf[:,'Mathematics']],axis=1)\n",
    "test_sta = pd.concat([preds_cb[:,'Statistics'],preds[:,'Statistics'],preds_lg[:,'Statistics'],preds_rf[:,'Statistics']],axis=1)\n",
    "test_qb = pd.concat([preds_cb[:,'Quantitative Biology'],preds[:,'Quantitative Biology'],preds_lg[:,'Quantitative Biology'],preds_rf[:,'Quantitative Biology']],axis=1)\n",
    "test_qf = pd.concat([preds_cb[:,'Quantitative Finance'],preds[:,'Quantitative Finance'],preds_lg[:,'Quantitative Finance'],preds_rf[:,'Quantitative Finance']],axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
